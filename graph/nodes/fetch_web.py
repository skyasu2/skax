"""
Fetch Web Context Node
"""
from graph.state import PlanCraftState, update_state
from graph.nodes.common import update_step_history
from tools.web_search_executor import execute_web_search
from utils.tracing import trace_node
from utils.error_handler import handle_node_error

@trace_node("context", tags=["web", "search", "tavily"])
@handle_node_error
def fetch_web_context(state: PlanCraftState) -> PlanCraftState:
    """
    ì¡°ê±´ë¶€ ì›¹ ì •ë³´ ìˆ˜ì§‘ ë…¸ë“œ

    [UPDATE] v1.5.0 - í”„ë¦¬ì…‹ ê¸°ë°˜ ì›¹ ê²€ìƒ‰ ìµœì í™”
    - fast: 1ê°œ ì¿¼ë¦¬, basic depth
    - balanced: 3ê°œ ì¿¼ë¦¬, basic depth
    - quality: 5ê°œ ì¿¼ë¦¬, advanced depth

    Side-Effect: ì™¸ë¶€ ì›¹ API í˜¸ì¶œ (Tavily Search)
    - ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ì•ˆì „í•¨ (ì¡°íšŒ ì „ìš©, ë©±ë“±ì„± ë³´ì¥)
    - ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€

    LangSmith: run_name="ğŸ“š ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘", tags=["rag", "retrieval", "web", "search", "tavily"]
    """
    import time
    from utils.settings import get_preset

    start_time = time.time()

    # [NEW] í”„ë¦¬ì…‹ ì„¤ì • ë¡œë“œ
    preset_key = state.get("generation_preset", "balanced")
    preset = get_preset(preset_key)

    # [NEW] ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” ì²´í¬
    if not preset.web_search_enabled:
        print(f"[FetchWeb] ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” (preset={preset_key})")
        return update_step_history(
            update_state(state, web_context=None, web_urls=[], web_sources=[]),
            "fetch_web", "SKIPPED", "ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”ë¨",
            start_time=start_time
        )

    user_input = state.get("user_input", "")
    rag_context = state.get("rag_context")
    
    # [NEW] Analyzer ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
    analysis = state.get("analysis")
    search_input = user_input
    
    if analysis:
        # Pydantic ëª¨ë¸ ë˜ëŠ” Dict ì²˜ë¦¬
        if hasattr(analysis, "model_dump"):
            analysis = analysis.model_dump()
            
        if isinstance(analysis, dict):
            topic = analysis.get("topic")
            if topic and topic != user_input:
                # ì£¼ì œê°€ ëª…í™•í•´ì¡Œìœ¼ë¯€ë¡œ ë” ì •í™•í•œ ì¿¼ë¦¬ ìƒì„± ê°€ëŠ¥
                # ì˜ˆ: "ê·¸ê±°..." -> "ìƒì„±í˜• AI íŠ¸ë Œë“œ"
                search_input = f"{topic} ì‹œì¥ ë™í–¥ ë° ì„±ê³µ ì‚¬ë¡€"
                print(f"[FetchWeb] ì¿¼ë¦¬ ìµœì í™”: '{user_input}' -> '{search_input}'")

    # 1. ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (Executor ìœ„ì„)
    # [NEW] í”„ë¦¬ì…‹ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ì „ë‹¬
    print(f"[FetchWeb] Preset={preset_key}, max_queries={preset.web_search_max_queries}, depth={preset.web_search_depth}")
    result = execute_web_search(
        search_input,
        rag_context,
        max_queries=preset.web_search_max_queries,
        search_depth=preset.web_search_depth
    )

    # [DEBUG] ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
    print(f"[FETCH_WEB DEBUG] urls={len(result.get('urls', []))}, sources={len(result.get('sources', []))}, context_len={len(result.get('context') or '')}, error={result.get('error')}")

    # 2. ìƒíƒœ ì—…ë°ì´íŠ¸
    existing_context = state.get("web_context")
    existing_urls = state.get("web_urls") or []
    existing_sources = state.get("web_sources") or []

    new_context_str = result["context"]
    new_urls = result["urls"]
    new_sources = result["sources"]
    error = result["error"]

    final_context = existing_context
    if new_context_str:
        final_context = f"{final_context}\n\n{new_context_str}" if final_context else new_context_str

    final_urls = list(dict.fromkeys(existing_urls + new_urls))

    # web_sources ë³‘í•© (ì¤‘ë³µ URL ì œê±°)
    final_sources = existing_sources.copy()
    for src in new_sources:
        if not any(s.get("url") == src.get("url") for s in final_sources):
            final_sources.append(src)

    if error:
        new_state = update_state(
            state,
            web_context=None,
            web_urls=[],
            error=f"ì›¹ ì¡°íšŒ ì˜¤ë¥˜: {error}"
        )
    else:
        new_state = update_state(
            state,
            web_context=final_context,
            web_urls=final_urls,
            web_sources=final_sources,
            current_step="fetch_web"
        )

    status = "FAILED" if new_state.get("error") else "SUCCESS"
    url_count = len(new_state.get("web_urls") or [])
    summary = f"ì›¹ ì •ë³´ ìˆ˜ì§‘: {url_count}ê°œ URL ì°¸ì¡°"
    
    return update_step_history(new_state, "fetch_web", status, summary, new_state.get("error"), start_time=start_time)
