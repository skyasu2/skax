# LLM Fundamentals 강의 자료 분석 요약

## 1. 강의 목적 요약
본 강의는 **LLM(Large Language Model)의 내부 동작 원리와 한계**를 이해시키는 것을 목표로 한다.  
단순히 “AI를 사용한다” 수준을 넘어, **왜 이런 응답이 나오는지 / 언제 위험한지**를 판단할 수 있는 기초 체력을 만드는 강의이다.

---

## 2. LLM의 정체: 무엇을 하는 모델인가

### 2.1 LLM의 본질
- LLM은 **다음 토큰(next token)을 예측하는 확률 모델**
- 지능, 이해, 사실 판단 능력을 *보장하지 않음*

📌 핵심 문장  
> LLM은 “생각”하지 않고, “가장 그럴듯한 문장”을 생성한다.

---

## 3. LLM 학습 방식 개요

### 3.1 사전 학습 (Pre-training)
- 대규모 텍스트 데이터를 통해 언어 패턴 학습
- 문법, 문맥, 일반 지식 습득

### 3.2 파인튜닝 (Fine-tuning)
- 특정 목적/도메인에 맞게 추가 학습
- 비용·시간 부담 큼

### 3.3 RLHF
- 인간 피드백 기반 보정
- “도움되는 답변처럼 보이게” 조정

---

## 4. 토큰(Token) 개념 이해

### 4.1 토큰이란
- 문장을 구성하는 최소 단위
- 단어 ≠ 토큰 (부분 문자열 포함)

### 4.2 토큰의 중요성
- 비용 산정 기준
- 입력 길이 제한
- 성능 및 응답 품질에 직접 영향

---

## 5. Context Window의 한계

### 5.1 Context Window란
- 한 번에 모델이 참고할 수 있는 토큰 수
- 초과 시 이전 정보 손실

### 5.2 실무 영향
- 긴 대화에서 맥락 붕괴
- 문서 전체 이해 불가

📌 해결 방향
- 요약
- Chunking
- RAG 도입

---

## 6. Temperature / Top-p 개념

### 6.1 Temperature
- 낮음: 안정적, 반복적
- 높음: 창의적, 불안정

### 6.2 Top-p
- 확률 상위 p% 토큰만 사용
- 출력 다양성 조절

📌 실무 권장
- 업무용 AI: 낮은 temperature
- 아이디어 생성: 중간 이상

---

## 7. Hallucination의 구조적 원인

### 7.1 발생 이유
- 사실 검증 능력 부재
- 문맥상 “그럴듯함” 우선

### 7.2 해결 전략
- Prompt 제약
- Output 검증
- RAG 사용

---

## 8. LLM의 한계 정리

- 최신 정보 모름
- 계산/논리 오류 가능
- 일관성 부족
- 장기 기억 없음

📌 따라서:
> LLM 단독 사용은 서비스에 위험

---

## 9. 왜 LLM Fundamentals가 중요한가

- Prompt Engineering 이해 기반
- RAG 필요성 자연스럽게 연결
- Agent 구조 설계 판단 가능
- “AI가 왜 틀렸는지” 설명 가능

---

## 10. 과제 및 프로젝트 적용 포인트

### 필수 이해 요소
- LLM은 확률 모델임을 명시
- Hallucination 대응 전략 언급
- Context 한계 인지

### 가점 요소
- 토큰/비용 고려 설계
- LLM 단독 vs RAG 비교 설명

---

## 11. 강의 핵심 메시지 요약

1. LLM은 지능이 아닌 확률 모델
2. 한계는 구조적이며 제거 불가
3. 설계로 보완해야 함
4. RAG/Agent는 필연적 진화
5. Fundamentals 이해가 모든 설계의 출발점

---

### 한 줄 결론
> **LLM Fundamentals는  
AI를 ‘믿지 않게 만드는’ 가장 중요한 강의다.**
