{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법은 가장 간단한 방식입니다.\n",
    "\n",
    "기본적으로 `\"\\n\\n\"` 을 기준으로 문자 단위로 텍스트를 분할하고, 청크의 크기를 문자 수로 측정합니다.\n",
    "\n",
    "1. 텍스트 분할 방식: 단일 문자 기준\n",
    "2. 청크 크기 측정 방식: 문자 수 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./data/textsplitter/appendix-keywords.txt\") as f:\n",
    "    file1 = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "with open(\"./data/textsplitter/finance-keywords.txt\") as f:\n",
    "    file2 = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter를 사용하여 분할\n",
    "\n",
    "CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드에 대해 설명합니다.\n",
    "\n",
    "- `separator`: 분할할 기준을 설정합니다. 기본값은 `\"\\n\\n\"`입니다.\n",
    "- `chunk_size`: 각 청크의 최대 크기를 설정합니다. 예: 250자\n",
    "- `chunk_overlap`: 인접한 청크 간 중복을 허용합니다. 예: 50자\n",
    "- `length_function`: 텍스트의 길이를 계산하는 함수를 지정합니다. 예: `len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 \"\\n\\n\"입니다.\n",
    "    separator=\"\\n\\n\",\n",
    "    # 분할된 텍스트 청크의 최대 크기를 지정합니다 (문자 수).\n",
    "    chunk_size=210,\n",
    "    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "    chunk_overlap=0,\n",
    "    # 텍스트의 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "20\n",
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 청크로 분할합니다.\n",
    "texts_1 = text_splitter.create_documents([file1])\n",
    "texts_2 = text_splitter.create_documents([file2])\n",
    "\n",
    "print(len(texts_1))\n",
    "print(len(texts_2))\n",
    "\n",
    "print(texts_1[0])  # 분할된 문서 중 첫 번째 문서를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 문서와 함께 메타데이터를 전달하는 예시입니다.\n",
    "\n",
    "메타데이터가 문서와 함께 분할되는 점에 주목해 주세요.\n",
    "\n",
    "- `create_documents` 메서드는 텍스트 데이터와 메타데이터 리스트를 인자로 받습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "page_content='정의: 방어주는 경기 변동에 상관없이 안정적인 실적을 보이는 기업의 주식을 의미합니다.\n",
      "예시: 프록터앤갬블, 존슨앤존슨과 같은 생활필수품 기업들은 S&P 500 내 대표적인 방어주로 꼽힙니다.\n",
      "연관키워드: 안정적 수익, 저변동성, 리스크 관리' metadata={'document': 2}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [\n",
    "    {\"document\": 1},\n",
    "    {\"document\": 2},\n",
    "]  # 문서에 대한 메타데이터 리스트를 정의합니다.\n",
    "documents = text_splitter.create_documents(\n",
    "    [\n",
    "        file1,\n",
    "        file2,\n",
    "    ],  # 분할할 텍스트 데이터를 리스트로 전달합니다.\n",
    "    metadatas=metadatas,  # 각 문서에 해당하는 메타데이터를 전달합니다.\n",
    ")\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[51])  # 분할된 문서 중 첫 번째 문서를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split_text()` 메서드를 사용하여 텍스트를 분할합니다.\n",
    "\n",
    "- `text_splitter.split_text(file)[0]`은 `file` 텍스트를 `text_splitter`를 사용하여 분할한 후, 분할된 텍스트 조각 중 첫 번째 요소를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding',\n",
       " '정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 분할하고, 분할된 텍스트의 첫 번째 요소를 반환합니다.\n",
    "text_splitter.split_text(file1)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 텍스트 분할기는 일반적인 텍스트에 권장되는 방식입니다.\n",
    "\n",
    "이 분할기는 문자 목록을 매개변수로 받아 동작합니다.\n",
    "\n",
    "분할기는 청크가 충분히 작아질 때까지 주어진 문자 목록의 순서대로 텍스트를 분할하려고 시도합니다.\n",
    "\n",
    "기본 문자 목록은 `[\"\\n\\n\", \"\\n\", \" \", \"\"]`입니다.\n",
    "\n",
    "- **단락** -> **문장** -> **단어** 순서로 재귀적으로 분할합니다.\n",
    "\n",
    "이는 단락(그 다음으로 문장, 단어) 단위가 의미적으로 가장 강하게 연관된 텍스트 조각으로 간주되므로, 가능한 한 함께 유지하려는 효과가 있습니다.\n",
    "\n",
    "1. 텍스트가 분할되는 방식: 문자 목록(`[\"\\n\\n\", \"\\n\", \" \", \"\"]`) 에 의해 분할됩니다.\n",
    "\n",
    "2. 청크 크기가 측정되는 방식: 문자 수에 의해 측정됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n",
    "- 읽어들인 내용을 `file` 변수에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./data/textsplitter/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter`를 사용하여 텍스트를 작은 청크로 분할하는 예제입니다.\n",
    "\n",
    "- `chunk_size`를 250 으로 설정하여 각 청크의 크기를 제한합니다.\n",
    "- `chunk_overlap`을 50 으로 설정하여 인접한 청크 간에 50 개 문자의 중첩을 허용합니다.\n",
    "- `length_function`으로 `len` 함수를 사용하여 텍스트의 길이를 계산합니다.\n",
    "- `is_separator_regex`를 `False`로 설정하여 구분자로 정규식을 사용하지 않습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 크기를 매우 작게 설정합니다. 예시를 위한 설정입니다.\n",
    "    chunk_size=250,\n",
    "    # 청크 간의 중복되는 문자 수를 설정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 문자열 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자로 정규식을 사용할지 여부를 설정합니다.\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n",
    "- 분할된 문서는 `texts` 리스트에 저장됩니다.\n",
    "- `print(texts[0])`과 `print(texts[1])`을 통해 분할된 문서의 첫 번째와 두 번째 문서를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n",
      "============================================================\n",
      "page_content='Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token'\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([file1])\n",
    "print(texts[0])  # 분할된 문서의 첫 번째 문서를 출력합니다.\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])  # 분할된 문서의 두 번째 문서를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `text_splitter.split_text()` 함수를 사용하여 `file` 텍스트를 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding',\n",
       " 'Embedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트를 분할하고 분할된 텍스트의 처음 2개 요소를 반환합니다.\n",
    "text_splitter.split_text(file1)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.\n",
    "\n",
    "`TokenTextSplitter` 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tiktoken\n",
    "\n",
    "`tiktoken` 은 OpenAI에서 만든 빠른 `Tokenizer` 입니다.\n",
    "\n",
    "- `./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.\n",
    "- 읽어들인 내용을 `file` 변수에 저장합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./data/textsplitter/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CharacterTextSplitter`를 사용하여 텍스트를 분할합니다.\n",
    "\n",
    "- `from_tiktoken_encoder` tiktoken 기반의 토큰화 방식을 사용하여 텍스트를 분할하는 메서드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # 청크 크기를 300으로 설정합니다.\n",
    "    chunk_size=300,\n",
    "    # 청크 간 중복되는 부분이 없도록 설정합니다.\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# file 텍스트를 청크 단위로 분할합니다.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분할된 청크의 개수를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))  # 분할된 청크의 개수를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Embedding'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- texts 리스트의 첫 번째 요소를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n"
     ]
    }
   ],
   "source": [
    "# texts 리스트의 첫 번째 요소를 출력합니다.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고**\n",
    "\n",
    "- `CharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우, 텍스트는 `CharacterTextSplitter`에 의해서만 분할되고 `tiktoken` 토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가 `tiktoken` 토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)\n",
    "\n",
    "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenTextSplitter\n",
    "\n",
    "- `TokenTextSplitter` 클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연�\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=300,  # 청크 크기를 10으로 설정합니다.\n",
    "    chunk_overlap=0,  # 청크 간 중복을 0으로 설정합니다.\n",
    ")\n",
    "\n",
    "# state_of_the_union 텍스트를 청크로 분할합니다.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # 분할된 텍스트의 첫 번째 청크를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemanticChunker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "텍스트를 의미론적 유사성에 기반하여 분할합니다.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 텍스트를 로드하고 내용을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./data/textsplitter/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemanticChunker 생성\n",
    "\n",
    "`SemanticChunker`는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.\n",
    "\n",
    "이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다.\n",
    "\n",
    "    `SemanticChunker`를 사용하여 텍스트를 의미적으로 관련된 청크로 분할합니다.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "text_splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분할\n",
    "\n",
    "- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분할된 청크를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n",
      "\n",
      "JSON\n",
      "\n",
      "정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n",
      "\n",
      "Transformer\n",
      "\n",
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다. 예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다. 연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "\n",
      "정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 분할된 청크 중 첫 번째 청크를 출력합니다.\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_documents()` 함수를 사용하여 청크를 문서로 변환할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n",
      "\n",
      "JSON\n",
      "\n",
      "정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n",
      "\n",
      "Transformer\n",
      "\n",
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다. 예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다. 연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "\n",
      "정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "print(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarkdownHeaderTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마크다운 파일의 구조를 이해하고 효율적으로 다루는 것은 문서 작업에 있어 매우 중요할 수 있습니다. 특히, 문서의 전체적인 맥락과 구조를 고려하여 의미 있는 방식으로 텍스트를 임베딩하는 과정은, 광범위한 의미와 주제를 더 잘 포착할 수 있는 포괄적인 벡터 표현을 생성하는 데 큰 도움이 됩니다.\n",
    "\n",
    "이러한 맥락에서, 마크다운 파일의 특정 부분, 즉 헤더별로 내용을 나누고 싶을 때가 있습니다. 예를 들어, 문서 내에서 각각의 헤더 아래에 있는 내용을 기반으로 서로 연관된 정보 덩어리, 즉 '청크'를 만들고 싶은 경우가 그러합니다. 이는 텍스트의 공통된 맥락을 유지하면서도, 문서의 구조적 요소를 효과적으로 활용하려는 시도입니다.\n",
    "\n",
    "이런 과제를 해결하기 위해, `MarkdownHeaderTextSplitter` 라는 도구를 활용할 수 있습니다. 이 도구는 문서를 지정된 헤더 집합에 따라 분할하여, 각 헤더 그룹 아래의 내용을 별도의 청크로 관리할 수 있게 합니다. 이 방법을 통해, 문서의 전반적인 구조를 유지하면서도 내용을 더 세밀하게 다룰 수 있게 되며, 이는 다양한 처리 과정에서 유용하게 활용될 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MarkdownHeaderTextSplitter`를 사용하여 마크다운 형식의 텍스트를 헤더 단위로 분할합니다.\n",
    "\n",
    "- 마크다운 문서의 헤더(`#`, `##`, `###` 등)를 기준으로 텍스트를 분할하는 역할을 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `markdown_document` 변수에 마크다운 형식의 문서가 할당됩니다.\n",
    "- `headers_to_split_on` 리스트에는 마크다운 헤더 레벨과 해당 레벨의 이름이 튜플 형태로 정의됩니다.\n",
    "- `MarkdownHeaderTextSplitter` 클래스를 사용하여 `markdown_splitter` 객체를 생성하며, `headers_to_split_on` 매개변수로 분할 기준이 되는 헤더 레벨을 전달합니다.\n",
    "- `split_text` 메서드를 호출하여 `markdown_document`를 헤더 레벨에 따라 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Title\n",
      "\n",
      "## 1. SubTitle\n",
      "\n",
      "Hi this is Jim\n",
      "\n",
      "Hi this is Joe\n",
      "\n",
      "### 1-1. Sub-SubTitle \n",
      "\n",
      "Hi this is Lance \n",
      "\n",
      "## 2. Baz\n",
      "\n",
      "Hi this is Molly\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# 마크다운 형식의 문서를 문자열로 정의합니다.\n",
    "markdown_document = \"# Title\\n\\n## 1. SubTitle\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n### 1-1. Sub-SubTitle \\n\\nHi this is Lance \\n\\n## 2. Baz\\n\\nHi this is Molly\"\n",
    "print(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi this is Jim  \n",
      "Hi this is Joe\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle'}\n",
      "=====================\n",
      "Hi this is Lance\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}\n",
      "=====================\n",
      "Hi this is Molly\n",
      "{'Header 1': 'Title', 'Header 2': '2. Baz'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [  # 문서를 분할할 헤더 레벨과 해당 레벨의 이름을 정의합니다.\n",
    "    (\n",
    "        \"#\",\n",
    "        \"Header 1\",\n",
    "    ),  # 헤더 레벨 1은 '#'로 표시되며, 'Header 1'이라는 이름을 가집니다.\n",
    "    (\n",
    "        \"##\",\n",
    "        \"Header 2\",\n",
    "    ),  # 헤더 레벨 2는 '##'로 표시되며, 'Header 2'라는 이름을 가집니다.\n",
    "    (\n",
    "        \"###\",\n",
    "        \"Header 3\",\n",
    "    ),  # 헤더 레벨 3은 '###'로 표시되며, 'Header 3'이라는 이름을 가집니다.\n",
    "]\n",
    "\n",
    "# 마크다운 헤더를 기준으로 텍스트를 분할하는 MarkdownHeaderTextSplitter 객체를 생성합니다.\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "# markdown_document를 헤더를 기준으로 분할하여 md_header_splits에 저장합니다.\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "## 1. SubTitle\n",
    "\n",
    "Hi this is Jim\n",
    "\n",
    "Hi this is Joe\n",
    "\n",
    "### 1-1. Sub-SubTitle \n",
    "\n",
    "Hi this is Lance \n",
    "\n",
    "## 2. Baz\n",
    "\n",
    "Hi this is Molly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 `MarkdownHeaderTextSplitter`는 분할되는 헤더를 출력 청크의 내용에서 제거합니다.\n",
    "\n",
    "이는 `strip_headers = False`로 설정하여 비활성화할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Title  \n",
      "## 1. SubTitle  \n",
      "Hi this is Jim  \n",
      "Hi this is Joe\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle'}\n",
      "=====================\n",
      "### 1-1. Sub-SubTitle  \n",
      "Hi this is Lance\n",
      "{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}\n",
      "=====================\n",
      "## 2. Baz  \n",
      "Hi this is Molly\n",
      "{'Header 1': 'Title', 'Header 2': '2. Baz'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    # 분할할 헤더를 지정합니다.\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    # 헤더를 제거하지 않도록 설정합니다.\n",
    "    strip_headers=False,\n",
    ")\n",
    "# 마크다운 문서를 헤더를 기준으로 분할합니다.\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 마크다운 그룹 내에서는 원하는 텍스트 분할기(text splitter)를 적용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro \n",
      "\n",
      "## History \n",
      "\n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n",
      "\n",
      "## Rise and divergence \n",
      "\n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n",
      "\n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n",
      "\n",
      "#### Standardization \n",
      "\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n",
      "\n",
      "# Implementations \n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n## History \\n\\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n## Rise and divergence \\n\\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n#### Standardization \\n\\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n# Implementations \\n\\nImplementations of Markdown are available for over a dozen programming languages.\"\n",
    "print(markdown_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, `MarkdownHeaderTextSplitter` 사용하여 마크다운 문서를 헤더를 기준으로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro  \n",
      "## History  \n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "## Rise and divergence  \n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "#### Standardization  \n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "# Implementations  \n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "{'Header 1': 'Implementations'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),  # 분할할 헤더 레벨과 해당 레벨의 이름을 지정합니다.\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# Markdown 문서를 헤더 레벨에 따라 분할합니다.\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 `MarkdownHeaderTextSplitter` 로 분할된 결과를 다시 `RecursiveCharacterTextSplitter` 로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Intro  \n",
      "## History\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "readers in its source code form.[9]\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "{'Header 1': 'Intro', 'Header 2': 'History'}\n",
      "=====================\n",
      "## Rise and divergence  \n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "#### Standardization\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n",
      "{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}\n",
      "=====================\n",
      "# Implementations  \n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "{'Header 1': 'Implementations'}\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 200  # 분할된 청크의 크기를 지정합니다.\n",
    "chunk_overlap = 20  # 분할된 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# 문서를 문자 단위로 분할합니다.\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
