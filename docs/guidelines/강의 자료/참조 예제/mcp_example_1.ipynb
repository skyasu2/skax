{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/gmsharpe/67eaa061d9c37ff447e64ba9bab21298/simple-mcp-example-using-ollama-medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkhgS1l98snn"
   },
   "source": [
    "# Anthropic의 Model Context Protocol (MCP) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLPIFZMixr2J"
   },
   "source": [
    "# 소개\n",
    "Model Context Protocol을 사용하여 간단한 서버와 클라이언트를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEiMddRWwMqz"
   },
   "source": [
    "### 종속성 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16305,
     "status": "ok",
     "timestamp": 1762385250278,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "2PtCNfGjwNQN",
    "outputId": "0690438e-ec4d-411c-90ef-af75a8e42fc4"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-openai mcp anyio click httpx uvicorn anyio --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKIrXKFZwrra"
   },
   "source": [
    "### Azure OpenAI 설정\n",
    "LangChain의 AzureOpenAI 클래스를 사용하여 Azure OpenAI 서비스에 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJRrEmdKx7yh"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4.1\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLF2hskV-hqV"
   },
   "outputs": [],
   "source": [
    "# 로깅 추가\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from mcp.client.session import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.CRITICAL,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.getLogger('httpcore').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('httpx').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('mcp').setLevel(logging.CRITICAL)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e20hRLeryAlZ"
   },
   "source": [
    "# 간단한 테스트로 연결 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4342,
     "status": "ok",
     "timestamp": 1762385269774,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "ybtzIA7gx9iB",
    "outputId": "a0e0d594-d0b6-479b-f741-6f97c859d430"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"오늘 뉴스 알려주세요\")]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqFo6gyRThaQ"
   },
   "source": [
    "---\n",
    "현재 아무런 tool 도 사용하지 않고 LLM에 '오늘 뉴스' 를 알려달라고 하면 예전의 정보를 알려줍니다.  \n",
    "LLM은 학습에 사용된 데이터까지의 정보만 알고있기 때문에, 최신 정보를 알 수 없기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAfxQo83yqHF"
   },
   "source": [
    "## Model Context Protocol (MCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjZWZNG2zrH6"
   },
   "source": [
    "### MCP 서버 생성 `[server.py]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1762385269806,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "NuKzajqlEcCZ",
    "outputId": "d083e3eb-2acb-413e-9447-061d76bd1d4e"
   },
   "outputs": [],
   "source": [
    "%%writefile server.py\n",
    "import mcp.types as types\n",
    "\n",
    "from mcp import Tool\n",
    "from mcp.server.sse import SseServerTransport\n",
    "from mcp.server import Server\n",
    "\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Route\n",
    "\n",
    "import uvicorn\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "app = Server(\"mcp-server\")\n",
    "sse = SseServerTransport(\"/messages\")\n",
    "\n",
    "port = 8010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLF2hskV-hqV"
   },
   "outputs": [],
   "source": [
    "%%writefile -a server.py\n",
    "\n",
    "# SSL 경고 무시 (선택사항)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Unverified HTTPS request')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UR_OF5J0a0E"
   },
   "source": [
    "#### 에이전트의 '도구'로 사용할 `fetch_website` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1762385269822,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "dbW96kfG7QA0",
    "outputId": "20b50700-a26d-4ec6-8827-d45351af3b61"
   },
   "outputs": [],
   "source": [
    "%%writefile -a server.py\n",
    "\n",
    "async def fetch_website(\n",
    "        url: str,\n",
    ") -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"MCP Test Server (github.com/modelcontextprotocol/python-sdk)\"\n",
    "    }\n",
    "    async with httpx.AsyncClient(follow_redirects=True, headers=headers, verify=False) as client:\n",
    "        response = await client.get(url)\n",
    "        response.raise_for_status()\n",
    "        return [types.TextContent(type=\"text\", text=response.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1762385269838,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "G2FtON_u5IZB",
    "outputId": "8522e332-2e61-423c-ccfb-4feef16661aa"
   },
   "outputs": [],
   "source": [
    "%%writefile -a server.py\n",
    "\n",
    "@app.call_tool()\n",
    "async def call_tool(\n",
    "  name: str, arguments: dict\n",
    ") -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n",
    "    if name == \"fetch\":\n",
    "        if \"url\" not in arguments:\n",
    "            raise ValueError(\"Missing required argument 'url'\")\n",
    "        return await fetch_website(arguments[\"url\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tool '{name}'\")\n",
    "\n",
    "@app.list_tools()\n",
    "async def list_tools() -> list[types.Tool]:\n",
    "    return [\n",
    "        types.Tool(\n",
    "            name=\"fetch\",\n",
    "            description=\"Fetches a website and returns its content\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"required\": [\"url\"],\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"URL to fetch\",\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46_99nCZ0o6D"
   },
   "source": [
    "#### MCP tool로 웹 검색을 하고 그 결과를 스트리밍하기 위한 별도 서버 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6ynIzsD_QnI"
   },
   "source": [
    "MCP의 데이터 통신 방식은 크게 두 가지 입니다.  \n",
    "하나는 stdio로, MCP 서버가 MCP host와 동일한 컴퓨터/환경에서 실행될 때 사용하는 방식입니다.  \n",
    "또 하나는 SSE (현재는 streamableHTTP 라는 방식으로 업그레이드 되었습니다) 방식으로, MCP 서버가 MCP host와 독립적으로 실행되는 경우입니다.  \n",
    "\n",
    "이번 예제에서는 SSE 방식을 사용하고, 로컬 환경에 MCP 서버 역할을 하는 별도의 서버를 띄워서 이 서버와 통신하는 방식으로 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1762385269877,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "fP4uVoow5XXH",
    "outputId": "27c2e3b1-5fd1-49d4-d553-b2ea9ce4ffc2"
   },
   "outputs": [],
   "source": [
    "%%writefile -a server.py\n",
    "\n",
    "async def handle_sse(request):\n",
    "    async with sse.connect_sse(\n",
    "            request.scope, request.receive, request._send\n",
    "    ) as streams:\n",
    "        await app.run(\n",
    "            streams[0], streams[1], app.create_initialization_options()\n",
    "        )\n",
    "\n",
    "async def handle_messages(request):\n",
    "    await sse.handle_post_message(request.scope, request.receive, request._send)\n",
    "\n",
    "starlette_app = Starlette(\n",
    "    debug=True,\n",
    "    routes=[\n",
    "        Route(\"/sse\", endpoint=handle_sse, methods=[\"GET\"]),\n",
    "        Route(\"/messages\", endpoint=handle_messages, methods=[\"POST\"]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# uvicorn으로 서버 실행\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(starlette_app, host=\"127.0.0.1\", port=port, log_level=\"error\", access_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie4UIGci3Upp"
   },
   "source": [
    "### 서버 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvxTOLBbEkn9"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mcp_server():\n",
    "    subprocess.Popen([\"python\", \"server.py\"])\n",
    "\n",
    "thread = threading.Thread(target=run_mcp_server)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXSbwuntuf49"
   },
   "outputs": [],
   "source": [
    "# MCP에서 반환된 도구 스키마를 Azure OpenAI 호환 스키마로 변환하는 함수 정의\n",
    "def convert_tool_format(tools):\n",
    "    \"\"\"\n",
    "    도구를 Azure OpenAI에 필요한 형식으로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        tools (list): 도구 객체 리스트\n",
    "\n",
    "    Returns:\n",
    "        dict: Azure OpenAI에 필요한 형식의 도구\n",
    "    \"\"\"\n",
    "    converted_tools = []\n",
    "\n",
    "    for tool in tools:\n",
    "        converted_tool = {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "                'name': tool.name,\n",
    "                'description': tool.description,\n",
    "                'parameters': tool.inputSchema\n",
    "            }\n",
    "        }\n",
    "\n",
    "        converted_tools.append(converted_tool)\n",
    "\n",
    "    return converted_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1762385303951,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "UqXddwJeaNqm",
    "outputId": "a1f73a82-ca1b-4518-e91f-d3829196033c"
   },
   "outputs": [],
   "source": [
    "# 연결 테스트\n",
    "# 서버가 떠있으면 Not found 에러가 반환되어 와야함\n",
    "!curl 127.0.0.1:8010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4oWeDzx14cI"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "async def call_tool(session, response, messages):\n",
    "    # 도구 사용 요청. 도구를 호출하고 결과를 모델에 전송합니다.\n",
    "    tool_calls = response.tool_calls\n",
    "    if not tool_calls:\n",
    "        raise ValueError(\"응답에서 도구 요청을 찾을 수 없습니다\")\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "\n",
    "        print(f\"\\n[도구 호출] {tool_name} - URL: {tool_args.get('url', 'N/A')}\")\n",
    "\n",
    "        try:\n",
    "            # MCP 세션을 통해 도구 호출\n",
    "            tool_response = await session.call_tool(tool_name, tool_args)\n",
    "\n",
    "            # 응답 길이 확인 (디버깅용)\n",
    "            response_str = str(tool_response)\n",
    "            print(f\"[도구 응답] 길이: {len(response_str)} 문자\")\n",
    "\n",
    "            # 도구 응답을 예상 형식으로 변환\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool_name + str(uuid.uuid4()),\n",
    "                \"content\": [{\"text\": response_str}]\n",
    "            }\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"[오류] 도구 호출 실패: {str(err)}\")\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool_call[\"id\"],\n",
    "                \"content\": [{\"text\": f\"오류: {str(err)}\"}],\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "        # 도구 결과를 메시지에 추가\n",
    "        from langchain_core.messages import ToolMessage\n",
    "        messages.append(ToolMessage(\n",
    "            content=str({\"toolResult\": tool_result}),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ))\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jFZuTCD1BJ6"
   },
   "outputs": [],
   "source": [
    "async def converse_using_azure_openai(session, messages, tools):\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        converted_tools = convert_tool_format(tools)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[반복 {iteration}] LLM 호출 중...\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Azure OpenAI에 도구를 바인딩하여 호출\n",
    "        llm_with_tools = llm.bind_tools(converted_tools)\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "\n",
    "        print(f\"\\n[LLM 응답]\")\n",
    "        if response.content:\n",
    "            # 응답이 너무 길면 잘라서 표시\n",
    "            content = response.content\n",
    "            if len(content) > 500:\n",
    "                print(f\"{content[:500]}... (총 {len(content)}자)\")\n",
    "            else:\n",
    "                print(content)\n",
    "        \n",
    "        if response.tool_calls:\n",
    "            print(f\"[도구 사용 요청] {len(response.tool_calls)}개의 도구 호출\")\n",
    "\n",
    "        messages.append(response)\n",
    "\n",
    "        if response.tool_calls:\n",
    "            await call_tool(session, response, messages)\n",
    "        else:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"✓ 완료: 더 이상 도구 사용 요청이 없습니다\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMLppWkRy06H"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "async def complete(message):\n",
    "    logger.info(\"세션 시작\")\n",
    "    messages = []\n",
    "\n",
    "    async with sse_client(\"http://localhost:8010/sse\") as streams:\n",
    "        async with ClientSession(streams[0], streams[1]) as session:\n",
    "            try:\n",
    "                await session.initialize()\n",
    "                # 세션이 초기화될 수 있도록 잠시 대기\n",
    "                await asyncio.sleep(1)\n",
    "                logger.info(\"세션 초기화 완료\")\n",
    "\n",
    "                # 사용 가능한 도구 목록을 가져오고 직렬화 가능한 형식으로 변환\n",
    "                tools_result = await session.list_tools()\n",
    "                tools_list = [{\"name\": tool.name, \"description\": tool.description,\n",
    "                              \"inputSchema\": tool.inputSchema} for tool in tools_result.tools]\n",
    "                logger.info(\"사용 가능한 도구: %s\", tools_list)\n",
    "\n",
    "                system_message = SystemMessage(content=f\"당신은 유용한 AI 어시스턴트입니다. 다음 도구를 사용할 수 있습니다: {json.dumps(tools_list, ensure_ascii=True)} 프롬프트(사용자)의 질문에 답하기 위해 필요한 경우 이 도구들을 사용하세요.\")\n",
    "                messages.append(system_message)\n",
    "                messages.append(message)\n",
    "            except TypeError as err:\n",
    "                logger.error(\"도구 호출 실패 - 오류: %s\", str(err))\n",
    "                tool_result = {\n",
    "                    \"content\": [{\"text\": f\"오류: {str(err)}\"}],\n",
    "                    \"status\": \"error\"\n",
    "                }\n",
    "            await converse_using_azure_openai(session, messages, tools_result.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14593,
     "status": "ok",
     "timestamp": 1762385324464,
     "user": {
      "displayName": "Jihwan Kim",
      "userId": "05438130622648529843"
     },
     "user_tz": -540
    },
    "id": "5xkaXMOBzNjj",
    "outputId": "d640e09f-109d-4de1-ecff-9f474b852c8a"
   },
   "outputs": [],
   "source": [
    "message = HumanMessage(content=\"오늘 ytn 뉴스 알려줘\")\n",
    "\n",
    "await complete(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voWOriD9GQ6p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
