{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e46434a",
   "metadata": {},
   "source": [
    "# OpenAIEmbeddings\n",
    "\n",
    "문서 임베딩은 문서의 내용을 수치적인 벡터로 변환하는 과정입니다. \n",
    "\n",
    "이 과정을 통해 문서의 의미를 수치화하고, 다양한 자연어 처리 작업에 활용할 수 있습니다. 대표적인 사전 학습된 언어 모델로는 BERT와 GPT가 있으며, 이러한 모델들은 문맥적 정보를 포착하여 문서의 의미를 인코딩합니다. \n",
    "\n",
    "문서 임베딩은 토큰화된 문서를 모델에 입력하여 임베딩 벡터를 생성하고, 이를 평균하여 전체 문서의 벡터를 생성합니다. 이 벡터는 문서 분류, 감성 분석, 문서 간 유사도 계산 등에 활용될 수 있습니다.\n",
    "\n",
    "[더 알아보기](https://platform.openai.com/docs/guides/embeddings/embedding-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8748488",
   "metadata": {},
   "source": [
    "## 설정\n",
    "\n",
    "먼저 langchain-openai를 설치하고 필요한 환경 변수를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fe79",
   "metadata": {},
   "source": [
    "지원되는 모델 목록\n",
    "\n",
    "| MODEL                  | PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAX INPUT | Embedding size |\n",
    "|------------------------|------------------|---------------------------|-----------|------------|\n",
    "| text-embedding-3-small | 62,500           | 62.3%                     | 8191      | 1536       |\n",
    "| text-embedding-3-large | 9,615            | 64.6%                     | 8191      | 3072       |\n",
    "| text-embedding-ada-002 | 12,500           | 61.0%                     | 8191      | 1536       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b6b47f-88d9-45b6-8de8-5a7d1ef8513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23453c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00541ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fbebb",
   "metadata": {},
   "source": [
    "## 쿼리 임베딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08285a94",
   "metadata": {},
   "source": [
    "`embeddings.embed_query(text)`는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.\n",
    "\n",
    "이 함수는 텍스트를 벡터 공간에 매핑하여 의미적으로 유사한 텍스트를 찾거나 텍스트 간의 유사도를 계산하는 데 사용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973c5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.\n",
    "query_result = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d86f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c80557",
   "metadata": {},
   "source": [
    "`query_result[:5]`는 `query_result` 리스트의 처음 5개 요소를 슬라이싱(slicing)하여 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af37ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00311934482306242,\n",
       " -0.007872357964515686,\n",
       " -0.012800268828868866,\n",
       " 0.030090242624282837,\n",
       " 0.016375118866562843]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리 결과의 처음 5개 항목을 선택합니다.\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f8fbc",
   "metadata": {},
   "source": [
    "## Document 임베딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fcf40",
   "metadata": {},
   "source": [
    "`embeddings.embed_documents()` 함수를 사용하여 텍스트 문서를 임베딩합니다.\n",
    "\n",
    "- `[text]`를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.\n",
    "- 함수 호출 결과로 반환된 임베딩 벡터를 `doc_result` 변수에 할당합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f5d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result = embeddings.embed_documents(\n",
    "    [text, text, text, text]\n",
    ")  # 텍스트를 임베딩하여 문서 벡터를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdee91",
   "metadata": {},
   "source": [
    "`doc_result[0][:5]`는 `doc_result` 리스트의 첫 번째 요소에서 처음 5개의 문자를 슬라이싱하여 선택합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c13b154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_result)  # 문서 벡터의 길이를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7565438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00311934482306242,\n",
       " -0.007872357964515686,\n",
       " -0.012800268828868866,\n",
       " 0.030090242624282837,\n",
       " 0.016375118866562843]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 결과의 첫 번째 요소에서 처음 5개 항목을 선택합니다.\n",
    "doc_result[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f47ff",
   "metadata": {},
   "source": [
    "## 차원 지정\n",
    "\n",
    "`text-embedding-3` 모델 클래스를 사용하면 반환되는 임베딩의 크기를 지정할 수 있습니다.\n",
    "\n",
    "예를 들어, 기본적으로 `text-embedding-3-small`는 1536 차원의 임베딩을 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb82bb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 결과의 첫 번째 요소의 길이를 반환합니다.\n",
    "len(doc_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60da32",
   "metadata": {},
   "source": [
    "### 차원(dimensions) 조정\n",
    "\n",
    "하지만 `dimensions=1024`를 전달함으로써 임베딩의 크기를 1024로 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfa8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화합니다.\n",
    "embeddings_1024 = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    dimensions=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3fbcdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주어진 텍스트를 임베딩하고 첫 번째 임베딩 벡터의 길이를 반환합니다.\n",
    "len(embeddings_1024.embed_documents([text])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8ca34",
   "metadata": {},
   "source": [
    "## 유사도 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d646ba-575b-4602-94ec-8f31c3d81762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/azureuser/services/jupyter/venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/azureuser/services/jupyter/venv/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/azureuser/services/jupyter/venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/azureuser/services/jupyter/venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/azureuser/services/jupyter/venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddbb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "sentence4 = \"Hi, nice to meet you.\"\n",
    "sentence5 = \"I like to eat apples.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8662aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentences = embeddings_1024.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9e24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fbf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유사도 0.9657] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
      "[유사도 0.8571] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.5445] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1725] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.8406] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.5340] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1875] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
      "[유사도 0.5869] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1596] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.2485] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "# sentence2 = \"안녕하세요? 만나서 반가워요.\"\n",
    "# sentence3 = \"Hi, nice to meet you.\"\n",
    "# sentence4 = \"I like to eat apples.\"\n",
    "\n",
    "for i, sentence in enumerate(embedded_sentences):\n",
    "    for j, other_sentence in enumerate(embedded_sentences):\n",
    "        if i < j:\n",
    "            print(\n",
    "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e424a",
   "metadata": {},
   "source": [
    "# Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f978e1d",
   "metadata": {},
   "source": [
    "이 노트북에서는 Chroma 벡터스토어를 시작하는 방법을 다룹니다.\n",
    "\n",
    "Chroma는 개발자의 생산성과 행복에 초점을 맞춘 AI 네이티브 오픈 소스 벡터 데이터베이스입니다. Chroma는 Apache 2.0에 따라 라이선스가 부여됩니다. \n",
    "\n",
    "\n",
    "**참고링크**\n",
    "\n",
    "- [Chroma LangChain 문서](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
    "- [Chroma 공식문서](https://docs.trychroma.com/getting-started)\n",
    "- [LangChain 지원 VectorStore 리스트](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6481c8",
   "metadata": {},
   "source": [
    "샘플 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f355dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038ec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n",
    "\n",
    "# 텍스트 파일을 load -> List[Document] 형태로 변환\n",
    "loader1 = TextLoader(\"./data/vector_store/nlp-keywords.txt\")\n",
    "loader2 = TextLoader(\"./data/vector_store/finance-keywords.txt\")\n",
    "\n",
    "# 문서 분할\n",
    "split_doc1 = loader1.load_and_split(text_splitter)\n",
    "split_doc2 = loader2.load_and_split(text_splitter)\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_doc1), len(split_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c393b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69df5c6",
   "metadata": {},
   "source": [
    "## VectorStore 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f584a",
   "metadata": {},
   "source": [
    "### 벡터 저장소 생성 (from_documents)\n",
    "\n",
    "`from_documents` 클래스 메서드는 문서 리스트로부터 벡터 저장소를 생성합니다. \n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `documents` (List[Document]): 벡터 저장소에 추가할 문서 리스트\n",
    "- `embedding` (Optional[Embeddings]): 임베딩 함수. 기본값은 None\n",
    "- `ids` (Optional[List[str]]): 문서 ID 리스트. 기본값은 None\n",
    "- `collection_name` (str): 생성할 컬렉션 이름.\n",
    "- `persist_directory` (Optional[str]): 컬렉션을 저장할 디렉토리. 기본값은 None\n",
    "- `client_settings` (Optional[chromadb.config.Settings]): Chroma 클라이언트 설정\n",
    "- `client` (Optional[chromadb.Client]): Chroma 클라이언트 인스턴스\n",
    "- `collection_metadata` (Optional[Dict]): 컬렉션 구성 정보. 기본값은 None\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `persist_directory`가 지정되면 컬렉션이 해당 디렉토리에 저장됩니다. 지정되지 않으면 데이터는 메모리에 임시로 저장됩니다.\n",
    "- 이 메서드는 내부적으로 `from_texts` 메서드를 호출하여 벡터 저장소를 생성합니다.\n",
    "- 문서의 `page_content`는 텍스트로, `metadata`는 메타데이터로 사용됩니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `Chroma`: 생성된 Chroma 벡터 저장소 인스턴스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1b28e",
   "metadata": {},
   "source": [
    "생성시 `documents` 매개변수로 `Document` 리스트를 전달합니다. embedding 에 활용할 임베딩 모델을 지정하며, `namespace` 의 역할을 하는 `collection_name` 을 지정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39159034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 생성\n",
    "db_doc = Chroma.from_documents(\n",
    "    documents=split_doc1, embedding=embeddings, collection_name=\"my_db2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd6aa6",
   "metadata": {},
   "source": [
    "`persist_directory` 지정시 disk 에 파일 형태로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 경로 지정\n",
    "DB_PATH = \"./chroma_db2\"\n",
    "\n",
    "# 문서를 디스크에 저장합니다. 저장시 persist_directory에 저장할 경로를 지정합니다.\n",
    "persist_db = Chroma.from_documents(\n",
    "    split_doc1, embeddings, persist_directory=DB_PATH, collection_name=\"my_db2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baabc65",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 `DB_PATH` 에 저장된 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디스크에서 문서를 로드합니다.\n",
    "persist_db = Chroma(\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"my_db2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3adda7",
   "metadata": {},
   "source": [
    "불러온 VectorStore 에서 저장된 데이터를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c6d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['771c0dd0-207d-43fe-b4e2-3b2405544a0b',\n",
       "  '4de1c04b-ff3c-4c28-89db-0b65f88a6ac3',\n",
       "  '0853b9f2-068e-4a74-a806-f48e62b7fe7d',\n",
       "  '871e7ffe-9817-4858-9e6c-874be323bbba',\n",
       "  '88b7ab46-943c-4eec-821a-4e2a8fc180bd',\n",
       "  'b7ec9d06-d270-476b-b750-ecfb72ce65e4',\n",
       "  'c5320686-5b75-41f4-a02a-0a06dfdb0c75',\n",
       "  '0d028c6c-cc22-4d53-8b1a-44f0ae2f878d',\n",
       "  '45bbcb56-2edc-4c94-b9a8-60d30448ffb2',\n",
       "  '0a852462-748d-4f3a-907f-7290495058f4',\n",
       "  'e1b08bce-2de1-4ca1-8ca2-b46639ed1490'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer',\n",
       "  '정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\\n예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nVectorStore\\n\\n정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화\\n\\nSQL\\n\\n정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\\n예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리\\n\\nCSV',\n",
       "  '정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace',\n",
       "  '정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\\n\\nDigital Transformation\\n\\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델\\n\\nCrawling\\n\\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\\n\\nWord2Vec',\n",
       "  '정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)\\n\\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\\n예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성\\n\\nFAISS (Facebook AI Similarity Search)\\n\\n정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\\n\\nOpen Source',\n",
       "  '정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)',\n",
       "  '정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame',\n",
       "  \"정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\\n예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\\n연관키워드: 데이터 분석, 판다스, 데이터 처리\\n\\nAttention 메커니즘\\n\\n정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서 사용됩니다.\\n예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\\n\\n판다스 (Pandas)\\n\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\",\n",
       "  'GPT (Generative Pretrained Transformer)\\n\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\n\\nInstructGPT\\n\\n정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\\n예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\\n\\nKeyword Search',\n",
       "  '정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\\n\\nPage Rank\\n\\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\\n\\n데이터 마이닝\\n\\n정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.\\n예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석\\n\\n멀티모달 (Multimodal)',\n",
       "  '정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'},\n",
       "  {'source': './data/vector_store/nlp-keywords.txt'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 저장된 데이터 확인\n",
    "persist_db.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a8a67",
   "metadata": {},
   "source": [
    "만약 `collection_name` 을 다르게 지정하면 저장된 데이터가 없기 때문에 아무런 결과도 얻지 못합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd3f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 디스크에서 문서를 로드합니다.\n",
    "persist_db2 = Chroma(\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"my_db3\",\n",
    ")\n",
    "\n",
    "# 저장된 데이터 확인\n",
    "persist_db2.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58aa85b",
   "metadata": {},
   "source": [
    "### 벡터 저장소 생성 (from_texts)\n",
    "\n",
    "`from_texts` 클래스 메서드는 텍스트 리스트로부터 벡터 저장소를 생성합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `texts` (List[str]): 컬렉션에 추가할 텍스트 리스트\n",
    "- `embedding` (Optional[Embeddings]): 임베딩 함수. 기본값은 None\n",
    "- `metadatas` (Optional[List[dict]]): 메타데이터 리스트. 기본값은 None\n",
    "- `ids` (Optional[List[str]]): 문서 ID 리스트. 기본값은 None\n",
    "- `collection_name` (str): 생성할 컬렉션 이름. 기본값은 '_LANGCHAIN_DEFAULT_COLLECTION_NAME'\n",
    "- `persist_directory` (Optional[str]): 컬렉션을 저장할 디렉토리. 기본값은 None\n",
    "- `client_settings` (Optional[chromadb.config.Settings]): Chroma 클라이언트 설정\n",
    "- `client` (Optional[chromadb.Client]): Chroma 클라이언트 인스턴스\n",
    "- `collection_metadata` (Optional[Dict]): 컬렉션 구성 정보. 기본값은 None\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `persist_directory`가 지정되면 컬렉션이 해당 디렉토리에 저장됩니다. 지정되지 않으면 데이터는 메모리에 임시로 저장됩니다.\n",
    "- `ids`가 제공되지 않으면 UUID를 사용하여 자동으로 생성됩니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- 생성된 벡터 저장소 인스턴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 리스트로 생성\n",
    "db_text = Chroma.from_texts(\n",
    "    [\"안녕하세요. 정말 반갑습니다.\", \"제 이름은 보경입니다.\"],\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"my_db4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9101f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['bfdf22c4-8e2d-43c0-b847-38765ab277e5',\n",
       "  'efa788cb-6c75-4dbe-9895-aaee498ed365',\n",
       "  'ef1aa2a5-fbd7-4f92-bc97-e2320c70afca',\n",
       "  'dca75dbe-a65e-485b-ac5a-4abfec15565f'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['안녕하세요. 정말 반갑습니다.',\n",
       "  '제 이름은 보경입니다.',\n",
       "  '안녕하세요. 정말 반갑습니다.',\n",
       "  '제 이름은 보경입니다.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None, None, None, None],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터를 조회합니다.\n",
    "db_text.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1273b50",
   "metadata": {},
   "source": [
    "### 유사도 검색\n",
    "\n",
    "`similarity_search` 메서드는 Chroma 데이터베이스에서 유사도 검색을 수행합니다. 이 메서드는 주어진 쿼리와 가장 유사한 문서들을 반환합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `query` (str): 검색할 쿼리 텍스트\n",
    "- `k` (int, 선택적): 반환할 결과의 수. 기본값은 4입니다.\n",
    "- `filter` (Dict[str, str], 선택적): 메타데이터로 필터링. 기본값은 None입니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `k` 값을 조절하여 원하는 수의 결과를 얻을 수 있습니다.\n",
    "- `filter` 매개변수를 사용하여 특정 메타데이터 조건에 맞는 문서만 검색할 수 있습니다.\n",
    "- 이 메서드는 점수 정보 없이 문서만 반환합니다. 점수 정보도 필요한 경우 `similarity_search_with_score` 메서드를 직접 사용하세요.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[Document]`: 쿼리 텍스트와 가장 유사한 문서들의 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64050253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fa3e44ce-3829-464d-b033-b1bc336225d5', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='f1ccf401-a562-4277-a1c8-232e608c4f79', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " Document(id='ebb24fd7-013a-446d-8ee5-327efc9e4900', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace'),\n",
       " Document(id='8aadc89b-61af-4455-8eaa-9a6bcb56f04f', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_doc.similarity_search(\"TF IDF 에 대하여 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e8553",
   "metadata": {},
   "source": [
    "`k` 값에 검색 결과의 개수를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fa3e44ce-3829-464d-b033-b1bc336225d5', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='f1ccf401-a562-4277-a1c8-232e608c4f79', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_doc.similarity_search(\"TF IDF 에 대하여 알려줘\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aeb9a5",
   "metadata": {},
   "source": [
    "`filter` 에 `metadata` 정보를 활용하여 검색 결과를 필터링 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd016a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fa3e44ce-3829-464d-b033-b1bc336225d5', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='f1ccf401-a562-4277-a1c8-232e608c4f79', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter 사용\n",
    "db_doc.similarity_search(\n",
    "    \"TF IDF 에 대하여 알려줘\", filter={\"source\": \"./data/vector_store/nlp-keywords.txt\"}, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aae2d3",
   "metadata": {},
   "source": [
    "다음은 `filter` 에서 다른 `source` 를 사용하여 검색한 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7e5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter 사용\n",
    "db_doc.similarity_search(\n",
    "    \"TF IDF 에 대하여 알려줘\", filter={\"source\": \"./data/vector_store/finance-keywords.txt\"}, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81905a16",
   "metadata": {},
   "source": [
    "### 벡터 저장소에 문서 추가\n",
    "\n",
    "`add_documents` 메서드는 벡터 저장소에 문서를 추가하거나 업데이트합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `documents` (List[Document]): 벡터 저장소에 추가할 문서 리스트\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "  - `ids`: 문서 ID 리스트 (제공 시 문서의 ID보다 우선함)\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `add_texts` 메서드가 구현되어 있어야 합니다.\n",
    "- 문서의 `page_content`는 텍스트로, `metadata`는 메타데이터로 사용됩니다.\n",
    "- 문서에 ID가 있고 `kwargs`에 ID가 제공되지 않으면 문서의 ID가 사용됩니다.\n",
    "- `kwargs`의 ID와 문서 수가 일치하지 않으면 ValueError가 발생합니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[str]`: 추가된 텍스트의 ID 리스트\n",
    "\n",
    "**예외**\n",
    "\n",
    "- `NotImplementedError`: `add_texts` 메서드가 구현되지 않은 경우 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# page_content, metadata, id 지정\n",
    "db_doc.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼께요\",\n",
    "            metadata={\"source\": \"mydata.txt\"},\n",
    "            id=\"1\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b4496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼께요'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'mydata.txt'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# id=1 로 문서 조회\n",
    "db_doc.get(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0b86c",
   "metadata": {},
   "source": [
    "`add_texts` 메서드는 텍스트를 임베딩하고 벡터 저장소에 추가합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `texts` (Iterable[str]): 벡터 저장소에 추가할 텍스트 리스트\n",
    "- `metadatas` (Optional[List[dict]]): 메타데이터 리스트. 기본값은 None\n",
    "- `ids` (Optional[List[str]]): 문서 ID 리스트. 기본값은 None\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `ids`가 제공되지 않으면 UUID를 사용하여 자동으로 생성됩니다.\n",
    "- 임베딩 함수가 설정되어 있으면 텍스트를 임베딩합니다.\n",
    "- 메타데이터가 제공된 경우:\n",
    "  - 메타데이터가 있는 텍스트와 없는 텍스트를 분리하여 처리합니다.\n",
    "  - 메타데이터가 없는 텍스트의 경우 빈 딕셔너리로 채웁니다.\n",
    "- 컬렉션에 upsert 작업을 수행하여 텍스트, 임베딩, 메타데이터를 추가합니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[str]`: 추가된 텍스트의 ID 리스트\n",
    "\n",
    "**예외**\n",
    "\n",
    "- `ValueError`: 복잡한 메타데이터로 인한 오류 발생 시, 필터링 방법 안내 메시지와 함께 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3b356",
   "metadata": {},
   "source": [
    "기존의 아이디에 추가하는 경우 `upsert` 가 수행되며, 기존의 문서는 대체됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77066510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 신규 데이터를 추가합니다. 이때 기존의 id=1 의 데이터는 덮어쓰게 됩니다.\n",
    "db_doc.add_texts(\n",
    "    [\"이전에 추가한 Document 를 덮어쓰겠습니다.\", \"덮어쓴 결과가 어떤가요?\"],\n",
    "    metadatas=[{\"source\": \"mydata.txt\"}, {\"source\": \"mydata.txt\"}],\n",
    "    ids=[\"1\", \"2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894137c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['이전에 추가한 Document 를 덮어쓰겠습니다.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'mydata.txt'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# id=1 조회\n",
    "db_doc.get([\"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5137",
   "metadata": {},
   "source": [
    "### 벡터 저장소에서 문서 삭제\n",
    "\n",
    "`delete` 메서드는 벡터 저장소에서 지정된 ID의 문서를 삭제합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `ids` (Optional[List[str]]): 삭제할 문서의 ID 리스트. 기본값은 None\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 이 메서드는 내부적으로 컬렉션의 `delete` 메서드를 호출합니다.\n",
    "- `ids`가 None이면 아무 작업도 수행하지 않습니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 1 삭제\n",
    "db_doc.delete(ids=[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cace65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['2'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['덮어쓴 결과가 어떤가요?'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'mydata.txt'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문서 조회\n",
    "db_doc.get([\"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f7f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['2'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['덮어쓴 결과가 어떤가요?'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'mydata.txt'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# where 조건으로 metadata 조회\n",
    "db_doc.get(where={\"source\": \"mydata.txt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f2fb4",
   "metadata": {},
   "source": [
    "### 초기화(reset_collection)\n",
    "\n",
    "`reset_collection` 메서드는 벡터 저장소의 컬렉션을 초기화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 초기화\n",
    "db_doc.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863c691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 초기화 후 문서 조회\n",
    "db_doc.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79ef44",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42bf6d",
   "metadata": {},
   "source": [
    "\n",
    "Facebook AI Similarity Search (Faiss)는 밀집 벡터의 효율적인 유사도 검색과 클러스터링을 위한 라이브러리입니다.\n",
    "\n",
    "Faiss는 RAM에 맞지 않을 수도 있는 벡터 집합을 포함하여 모든 크기의 벡터 집합을 검색하는 알고리즘을 포함하고 있습니다.\n",
    "\n",
    "또한 평가와 매개변수 튜닝을 위한 지원 코드도 포함되어 있습니다.\n",
    "\n",
    "**참고**\n",
    "- [LangChain FAISS 문서](https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/)\n",
    "- [FAISS 문서](https://faiss.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec3b8a",
   "metadata": {},
   "source": [
    "샘플 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e22325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n",
    "\n",
    "# 텍스트 파일을 load -> List[Document] 형태로 변환\n",
    "loader1 = TextLoader(\"./data/vector_store/nlp-keywords.txt\")\n",
    "loader2 = TextLoader(\"./data/vector_store/finance-keywords.txt\")\n",
    "\n",
    "# 문서 분할\n",
    "split_doc1 = loader1.load_and_split(text_splitter)\n",
    "split_doc2 = loader2.load_and_split(text_splitter)\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_doc1), len(split_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b35a4",
   "metadata": {},
   "source": [
    "### FAISS 벡터 저장소 생성 (from_documents)\n",
    "\n",
    "`from_documents` 클래스 메서드는 문서 리스트와 임베딩 함수를 사용하여 FAISS 벡터 저장소를 생성합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `documents` (List[Document]): 벡터 저장소에 추가할 문서 리스트\n",
    "- `embedding` (Embeddings): 사용할 임베딩 함수\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 문서 리스트에서 텍스트 내용(`page_content`)과 메타데이터를 추출합니다.\n",
    "2. 추출한 텍스트와 메타데이터를 사용하여 `from_texts` 메서드를 호출합니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `VectorStore`: 문서와 임베딩으로 초기화된 벡터 저장소 인스턴스\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 이 메서드는 `from_texts` 메서드를 내부적으로 호출하여 벡터 저장소를 생성합니다.\n",
    "- 문서의 `page_content`는 텍스트로, `metadata`는 메타데이터로 사용됩니다.\n",
    "- 추가적인 설정이 필요한 경우 `kwargs`를 통해 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /jupyter/dvenv/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /jupyter/dvenv/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /jupyter/dvenv/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 생성\n",
    "split_doc1.extend(split_doc2)\n",
    "db_doc = FAISS.from_documents(documents=split_doc1, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '76c6fb3c-b5d2-4dd3-8c15-4b69151472c9',\n",
       " 1: 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f',\n",
       " 2: '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f',\n",
       " 3: 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19',\n",
       " 4: 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a',\n",
       " 5: 'abfd77cf-f48b-47b1-80b9-20ca10c77068',\n",
       " 6: 'f297e148-3c06-4cab-bc1b-84d4fd428b99',\n",
       " 7: 'b5b31528-c28e-43f8-9598-0c3637fa4c1d',\n",
       " 8: '8641f8e4-9888-405b-a3e7-ee40b7950d6f',\n",
       " 9: 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa',\n",
       " 10: 'ab324b30-7bc1-455a-955d-8025fa63b452',\n",
       " 11: '0fdfb702-0f47-46fc-aead-6ee7d621ebae',\n",
       " 12: '97ee36e7-dc58-4ba9-a661-ac5661f36619',\n",
       " 13: '0b22ee18-c720-4aee-bb0b-38b5c666a644',\n",
       " 14: 'b2b4ba5f-992c-466a-80b9-caa14110adaa',\n",
       " 15: '8a7e207f-71c9-4f88-8b47-9dfbab576420',\n",
       " 16: 'b86860c0-2596-4129-8bb2-a572de594720'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문서 저장소 ID 확인\n",
    "db_doc.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dd515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'76c6fb3c-b5d2-4dd3-8c15-4b69151472c9': Document(id='76c6fb3c-b5d2-4dd3-8c15-4b69151472c9', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer'),\n",
       " 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f': Document(id='ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\\n예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nVectorStore\\n\\n정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화\\n\\nSQL\\n\\n정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\\n예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리\\n\\nCSV'),\n",
       " '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f': Document(id='212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace'),\n",
       " 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19': Document(id='ac5bac5b-b907-41d5-8c05-4f3e25c03b19', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\\n\\nDigital Transformation\\n\\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델\\n\\nCrawling\\n\\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\\n\\nWord2Vec'),\n",
       " 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a': Document(id='c810ff52-2abb-48a3-90c5-ffb6e61d9b4a', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)\\n\\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\\n예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성\\n\\nFAISS (Facebook AI Similarity Search)\\n\\n정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\\n\\nOpen Source'),\n",
       " 'abfd77cf-f48b-47b1-80b9-20ca10c77068': Document(id='abfd77cf-f48b-47b1-80b9-20ca10c77068', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " 'f297e148-3c06-4cab-bc1b-84d4fd428b99': Document(id='f297e148-3c06-4cab-bc1b-84d4fd428b99', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " 'b5b31528-c28e-43f8-9598-0c3637fa4c1d': Document(id='b5b31528-c28e-43f8-9598-0c3637fa4c1d', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content=\"정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\\n예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\\n연관키워드: 데이터 분석, 판다스, 데이터 처리\\n\\nAttention 메커니즘\\n\\n정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서 사용됩니다.\\n예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\\n\\n판다스 (Pandas)\\n\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\"),\n",
       " '8641f8e4-9888-405b-a3e7-ee40b7950d6f': Document(id='8641f8e4-9888-405b-a3e7-ee40b7950d6f', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='GPT (Generative Pretrained Transformer)\\n\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\n\\nInstructGPT\\n\\n정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\\n예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\\n\\nKeyword Search'),\n",
       " 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa': Document(id='ad7a237f-e9b7-4acb-97f4-81eb8ae524aa', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\\n\\nPage Rank\\n\\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\\n\\n데이터 마이닝\\n\\n정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.\\n예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석\\n\\n멀티모달 (Multimodal)'),\n",
       " 'ab324b30-7bc1-455a-955d-8025fa63b452': Document(id='ab324b30-7bc1-455a-955d-8025fa63b452', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝'),\n",
       " '0fdfb702-0f47-46fc-aead-6ee7d621ebae': Document(id='0fdfb702-0f47-46fc-aead-6ee7d621ebae', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='S&P 500\\n\\n정의: S&P 500은 미국 주식 시장에 상장된 500개의 대형 기업의 주가를 종합한 지수입니다. 이는 미국 경제와 주식 시장의 전반적인 상황을 나타내는 주요 지표로 사용됩니다.\\n예시: 애플, 마이크로소프트, 아마존과 같은 대형 기술 기업들이 S&P 500에 포함되어 있습니다.\\n연관키워드: 주식 시장, 지수, 대형주\\n\\nMarket Capitalization\\n\\n정의: 시가총액은 회사의 발행 주식 수와 현재 주가를 곱한 값으로, 회사의 전체 가치를 나타냅니다.\\n예시: 애플의 시가총액이 2조 달러를 넘어서면서 S&P 500 지수에서 가장 큰 비중을 차지하게 되었습니다.\\n연관키워드: 기업 가치, 주식, 투자\\n\\nDividend\\n\\n정의: 배당금은 기업이 주주들에게 이익의 일부를 현금으로 지급하는 것을 말합니다.\\n예시: 코카콜라는 50년 이상 연속으로 배당금을 인상해온 S&P 500 기업 중 하나입니다.\\n연관키워드: 주주 가치, 수익률, 투자 전략\\n\\nBlue Chip Stocks'),\n",
       " '97ee36e7-dc58-4ba9-a661-ac5661f36619': Document(id='97ee36e7-dc58-4ba9-a661-ac5661f36619', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: 블루칩 주식은 재무적으로 안정적이고 오랜 기간 동안 꾸준한 실적을 보여온 대형 기업의 주식을 의미합니다.\\n예시: 존슨앤존슨, 프록터앤갬블과 같은 기업들은 S&P 500에 포함된 대표적인 블루칩 주식입니다.\\n연관키워드: 안정적 투자, 대형주, 배당주\\n\\nSector Rotation\\n\\n정의: 섹터 로테이션은 투자자들이 경제 사이클에 따라 다른 산업 섹터로 투자를 이동시키는 전략을 말합니다.\\n예시: 경기 회복기에 투자자들이 기술주에서 금융주로 투자를 이동시키는 것이 섹터 로테이션의 한 예입니다.\\n연관키워드: 투자 전략, 자산 배분, 경제 사이클\\n\\nEarnings Per Share (EPS)\\n\\n정의: 주당순이익(EPS)은 기업의 순이익을 발행 주식 수로 나눈 값으로, 주주들에게 돌아가는 이익을 나타냅니다.\\n예시: 테슬라의 EPS가 예상치를 상회하면서 주가가 급등했습니다.\\n연관키워드: 재무 지표, 기업 실적, 주식 가치평가\\n\\nPrice-to-Earnings Ratio (P/E Ratio)'),\n",
       " '0b22ee18-c720-4aee-bb0b-38b5c666a644': Document(id='0b22ee18-c720-4aee-bb0b-38b5c666a644', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: 주가수익비율(P/E)은 주가를 주당순이익으로 나눈 값으로, 기업의 가치를 평가하는 데 사용되는 지표입니다.\\n예시: 아마존의 P/E 비율이 높은 것은 투자자들이 회사의 미래 성장 가능성을 높게 평가하고 있다는 것을 의미합니다.\\n연관키워드: 주식 가치평가, 투자 분석, 성장주\\n\\nQuarterly Earnings Report\\n\\n정의: 분기별 실적 보고서는 기업이 3개월마다 발표하는 재무 성과와 사업 현황에 대한 보고서입니다.\\n예시: 애플의 분기별 실적 발표는 전체 기술 섹터와 S&P 500 지수에 큰 영향을 미칩니다.\\n연관키워드: 기업 실적, 투자자 관계, 재무 분석\\n\\nIndex Fund\\n\\n정의: 인덱스 펀드는 S&P 500과 같은 특정 지수의 구성과 성과를 그대로 추종하는 투자 상품입니다.\\n예시: 바운가드 S&P 500 ETF는 S&P 500 지수를 추종하는 대표적인 인덱스 펀드입니다.\\n연관키워드: 패시브 투자, ETF, 포트폴리오 관리\\n\\nMarket Weight'),\n",
       " 'b2b4ba5f-992c-466a-80b9-caa14110adaa': Document(id='b2b4ba5f-992c-466a-80b9-caa14110adaa', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: 시장 가중치는 특정 기업이나 섹터가 전체 지수에서 차지하는 비중을 나타냅니다.\\n예시: 기술 섹터는 S&P 500 지수에서 가장 큰 시장 가중치를 차지하고 있습니다.\\n연관키워드: 포트폴리오 구성, 섹터 분석, 자산 배분\\n\\nGrowth Stock\\n\\n정의: 성장주는 평균 이상의 높은 성장률을 보이는 기업의 주식을 의미합니다.\\n예시: 페이스북(메타)과 같은 기술 기업들은 S&P 500에 포함된 대표적인 성장주로 꼽힙니다.\\n연관키워드: 고성장 기업, 기술주, 투자 전략\\n\\nValue Stock\\n\\n정의: 가치주는 현재 시장 가치가 내재 가치보다 낮다고 평가되는 기업의 주식을 말합니다.\\n예시: 워렌 버핏이 투자한 코카콜라는 S&P 500에 포함된 대표적인 가치주 중 하나입니다.\\n연관키워드: 가치 투자, 배당주, 안정적 수익\\n\\nMarket Volatility\\n\\n정의: 시장 변동성은 주식 시장의 가격 변동 폭을 나타내는 지표입니다.\\n예시: VIX 지수(변동성 지수)가 상승하면 S&P 500 지수의 변동성이 높아질 것으로 예상됩니다.\\n연관키워드: 리스크 관리, 투자 심리, 헤지 전략\\n\\nEquity Research'),\n",
       " '8a7e207f-71c9-4f88-8b47-9dfbab576420': Document(id='8a7e207f-71c9-4f88-8b47-9dfbab576420', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: 주식 리서치는 기업의 재무 상태, 사업 모델, 경쟁력 등을 분석하여 투자 의사 결정을 돕는 활동입니다.\\n예시: 골드만삭스의 애널리스트들이 S&P 500 기업들에 대한 분기별 실적 전망을 발표했습니다.\\n연관키워드: 투자 분석, 기업 가치평가, 시장 전망\\n\\nCorporate Governance\\n\\n정의: 기업 지배구조는 기업의 경영과 통제에 관한 시스템과 프로세스를 의미합니다.\\n예시: S&P 500 기업들 중 이사회의 다양성을 높이는 기업들이 증가하고 있습니다.\\n연관키워드: 주주 권리, ESG, 기업 윤리\\n\\nMergers and Acquisitions (M&A)\\n\\n정의: 인수합병은 기업들이 다른 기업을 사거나 합치는 과정을 말합니다.\\n예시: 마이크로소프트가 액티비전 블리자드를 인수하면서 S&P 500 내 게임 산업의 판도가 변화했습니다.\\n연관키워드: 기업 전략, 시너지 효과, 기업 가치\\n\\nESG (Environmental, Social, and Governance)'),\n",
       " 'b86860c0-2596-4129-8bb2-a572de594720': Document(id='b86860c0-2596-4129-8bb2-a572de594720', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: ESG는 기업의 환경, 사회, 지배구조 측면을 고려하는 투자 접근 방식입니다.\\n예시: S&P 500 ESG 지수는 우수한 ESG 성과를 보이는 기업들로 구성된 지수입니다.\\n연관키워드: 지속가능 투자, 기업의 사회적 책임, 윤리 경영\\n\\nStock Buyback\\n\\n정의: 자사주 매입은 기업이 자사의 주식을 시장에서 다시 사들이는 것을 말합니다.\\n예시: 애플은 S&P 500 기업 중 가장 큰 규모의 자사주 매입 프로그램을 운영하고 있습니다.\\n연관키워드: 주주 가치, 자본 관리, 주가 부양\\n\\nCyclical Stocks\\n\\n정의: 경기순환주는 경제 상황에 따라 실적이 크게 변동하는 기업의 주식을 말합니다.\\n예시: 포드, 제너럴 모터스와 같은 자동차 기업들은 S&P 500에 포함된 대표적인 경기순환주입니다.\\n연관키워드: 경제 사이클, 섹터 분석, 투자 타이밍\\n\\nDefensive Stocks\\n\\n정의: 방어주는 경기 변동에 상관없이 안정적인 실적을 보이는 기업의 주식을 의미합니다.\\n예시: 프록터앤갬블, 존슨앤존슨과 같은 생활필수품 기업들은 S&P 500 내 대표적인 방어주로 꼽힙니다.\\n연관키워드: 안정적 수익, 저변동성, 리스크 관리')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 저장된 문서의 ID: Document 확인\n",
    "db_doc.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cab7a",
   "metadata": {},
   "source": [
    "### FAISS 벡터 저장소 생성 (from_texts)\n",
    "\n",
    "`from_texts` 클래스 메서드는 텍스트 리스트와 임베딩 함수를 사용하여 FAISS 벡터 저장소를 생성합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `texts` (List[str]): 벡터 저장소에 추가할 텍스트 리스트\n",
    "- `embedding` (Embeddings): 사용할 임베딩 함수\n",
    "- `metadatas` (Optional[List[dict]]): 메타데이터 리스트. 기본값은 None\n",
    "- `ids` (Optional[List[str]]): 문서 ID 리스트. 기본값은 None\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 제공된 임베딩 함수를 사용하여 텍스트를 임베딩합니다.\n",
    "2. 임베딩된 벡터와 함께 `__from` 메서드를 호출하여 FAISS 인스턴스를 생성합니다.\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `FAISS`: 생성된 FAISS 벡터 저장소 인스턴스\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 이 메서드는 사용자 친화적인 인터페이스로, 문서 임베딩, 메모리 내 문서 저장소 생성, FAISS 데이터베이스 초기화를 한 번에 처리합니다.\n",
    "- 빠르게 시작하기 위한 편리한 방법입니다.\n",
    "\n",
    "**주의사항**\n",
    "\n",
    "- 대량의 텍스트를 처리할 때는 메모리 사용량에 주의해야 합니다.\n",
    "- 메타데이터나 ID를 사용하려면 텍스트 리스트와 동일한 길이의 리스트로 제공해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 리스트로 생성\n",
    "db_text = FAISS.from_texts(\n",
    "    [\"안녕하세요. 정말 반갑습니다.\", \"제 이름은 보경입니다.\"],\n",
    "    embedding=embeddings,\n",
    "    metadatas=[{\"source\": \"텍스트문서\"}, {\"source\": \"텍스트문서\"}],\n",
    "    ids=[\"doc1\", \"doc2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44924500",
   "metadata": {},
   "source": [
    "저장된 결과를 확인합니다. id 값은 지정한 id 값이 잘 들어가 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff37314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': Document(id='doc1', metadata={'source': '텍스트문서'}, page_content='안녕하세요. 정말 반갑습니다.'),\n",
       " 'doc2': Document(id='doc2', metadata={'source': '텍스트문서'}, page_content='제 이름은 보경입니다.')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 저장된 내용\n",
    "db_text.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1facdb",
   "metadata": {},
   "source": [
    "### 유사도 검색 (Similarity Search)\n",
    "\n",
    "`similarity_search` 메서드는 주어진 쿼리와 가장 유사한 문서들을 검색하는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `query` (str): 유사한 문서를 찾기 위한 검색 쿼리 텍스트\n",
    "- `k` (int): 반환할 문서 수. 기본값은 4\n",
    "- `filter` (Optional[Union[Callable, Dict[str, Any]]]): 메타데이터 필터링 함수 또는 딕셔너리. 기본값은 None\n",
    "- `fetch_k` (int): 필터링 전에 가져올 문서 수. 기본값은 20\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[Document]`: 쿼리와 가장 유사한 문서 리스트\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. `similarity_search_with_score` 메서드를 내부적으로 호출하여 유사도 점수와 함께 문서를 검색합니다.\n",
    "2. 검색 결과에서 점수를 제외하고 문서만 추출하여 반환합니다.\n",
    "\n",
    "**주요 특징**\n",
    "\n",
    "- `filter` 매개변수를 사용하여 메타데이터 기반의 필터링이 가능합니다.\n",
    "- `fetch_k`를 통해 필터링 전 검색할 문서 수를 조절할 수 있어, 필터링 후 원하는 수의 문서를 확보할 수 있습니다.\n",
    "\n",
    "**사용 시 고려사항**\n",
    "\n",
    "- 검색 성능은 사용된 임베딩 모델의 품질에 크게 의존합니다.\n",
    "- 대규모 데이터셋에서는 `k`와 `fetch_k` 값을 적절히 조정하여 검색 속도와 정확도의 균형을 맞추는 것이 중요합니다.\n",
    "- 복잡한 필터링이 필요한 경우, `filter` 매개변수에 커스텀 함수를 전달하여 세밀한 제어가 가능합니다.\n",
    "\n",
    "**최적화 팁**\n",
    "\n",
    "- 자주 사용되는 쿼리에 대해서는 결과를 캐싱하여 반복적인 검색 속도를 향상시킬 수 있습니다.\n",
    "- `fetch_k`를 너무 크게 설정하면 검색 속도가 느려질 수 있으므로, 적절한 값을 실험적으로 찾는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4498185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f297e148-3c06-4cab-bc1b-84d4fd428b99', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='abfd77cf-f48b-47b1-80b9-20ca10c77068', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " Document(id='212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace'),\n",
       " Document(id='76c6fb3c-b5d2-4dd3-8c15-4b69151472c9', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 유사도 검색\n",
    "db_doc.similarity_search(\"TF IDF 에 대하여 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070af73f",
   "metadata": {},
   "source": [
    "`k` 값에 검색 결과의 개수를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f297e148-3c06-4cab-bc1b-84d4fd428b99', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='abfd77cf-f48b-47b1-80b9-20ca10c77068', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# k 값 지정\n",
    "db_doc.similarity_search(\"TF IDF 에 대하여 알려줘\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ccc95",
   "metadata": {},
   "source": [
    "filter 에 metadata 정보를 활용하여 Filtering 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d6ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f297e148-3c06-4cab-bc1b-84d4fd428b99', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(id='abfd77cf-f48b-47b1-80b9-20ca10c77068', metadata={'source': './data/vector_store/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter 사용\n",
    "db_doc.similarity_search(\n",
    "    \"TF IDF 에 대하여 알려줘\", filter={\"source\": \"./data/vector_store/nlp-keywords.txt\"}, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878812a",
   "metadata": {},
   "source": [
    "다음은 `filter` 에서 다른 `source` 를 사용하여 검색한 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b17187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0b22ee18-c720-4aee-bb0b-38b5c666a644', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: 주가수익비율(P/E)은 주가를 주당순이익으로 나눈 값으로, 기업의 가치를 평가하는 데 사용되는 지표입니다.\\n예시: 아마존의 P/E 비율이 높은 것은 투자자들이 회사의 미래 성장 가능성을 높게 평가하고 있다는 것을 의미합니다.\\n연관키워드: 주식 가치평가, 투자 분석, 성장주\\n\\nQuarterly Earnings Report\\n\\n정의: 분기별 실적 보고서는 기업이 3개월마다 발표하는 재무 성과와 사업 현황에 대한 보고서입니다.\\n예시: 애플의 분기별 실적 발표는 전체 기술 섹터와 S&P 500 지수에 큰 영향을 미칩니다.\\n연관키워드: 기업 실적, 투자자 관계, 재무 분석\\n\\nIndex Fund\\n\\n정의: 인덱스 펀드는 S&P 500과 같은 특정 지수의 구성과 성과를 그대로 추종하는 투자 상품입니다.\\n예시: 바운가드 S&P 500 ETF는 S&P 500 지수를 추종하는 대표적인 인덱스 펀드입니다.\\n연관키워드: 패시브 투자, ETF, 포트폴리오 관리\\n\\nMarket Weight'),\n",
       " Document(id='b86860c0-2596-4129-8bb2-a572de594720', metadata={'source': './data/vector_store/finance-keywords.txt'}, page_content='정의: ESG는 기업의 환경, 사회, 지배구조 측면을 고려하는 투자 접근 방식입니다.\\n예시: S&P 500 ESG 지수는 우수한 ESG 성과를 보이는 기업들로 구성된 지수입니다.\\n연관키워드: 지속가능 투자, 기업의 사회적 책임, 윤리 경영\\n\\nStock Buyback\\n\\n정의: 자사주 매입은 기업이 자사의 주식을 시장에서 다시 사들이는 것을 말합니다.\\n예시: 애플은 S&P 500 기업 중 가장 큰 규모의 자사주 매입 프로그램을 운영하고 있습니다.\\n연관키워드: 주주 가치, 자본 관리, 주가 부양\\n\\nCyclical Stocks\\n\\n정의: 경기순환주는 경제 상황에 따라 실적이 크게 변동하는 기업의 주식을 말합니다.\\n예시: 포드, 제너럴 모터스와 같은 자동차 기업들은 S&P 500에 포함된 대표적인 경기순환주입니다.\\n연관키워드: 경제 사이클, 섹터 분석, 투자 타이밍\\n\\nDefensive Stocks\\n\\n정의: 방어주는 경기 변동에 상관없이 안정적인 실적을 보이는 기업의 주식을 의미합니다.\\n예시: 프록터앤갬블, 존슨앤존슨과 같은 생활필수품 기업들은 S&P 500 내 대표적인 방어주로 꼽힙니다.\\n연관키워드: 안정적 수익, 저변동성, 리스크 관리')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter 사용\n",
    "db_doc.similarity_search(\n",
    "    \"TF IDF 에 대하여 알려줘\", filter={\"source\": \"./data/vector_store/finance-keywords.txt\"}, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110c06e",
   "metadata": {},
   "source": [
    "### 문서(Document)로부터 추가 (add_documents)\n",
    "\n",
    "`add_documents` 메서드는 벡터 저장소에 문서를 추가하거나 업데이트하는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `documents` (List[Document]): 벡터 저장소에 추가할 문서 리스트\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[str]`: 추가된 텍스트의 ID 리스트\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 문서에서 텍스트 내용과 메타데이터를 추출합니다.\n",
    "2. `add_texts` 메서드를 호출하여 실제 추가 작업을 수행합니다.\n",
    "\n",
    "**주요 특징**\n",
    "\n",
    "- 문서 객체를 직접 처리할 수 있어 편리합니다.\n",
    "- ID 처리 로직이 포함되어 있어 문서의 고유성을 보장합니다.\n",
    "- `add_texts` 메서드를 기반으로 동작하여 코드 재사용성을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f98517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_doc1']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# page_content, metadata 지정\n",
    "db_doc.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼께요\",\n",
    "            metadata={\"source\": \"mydata.txt\"},\n",
    "        )\n",
    "    ],\n",
    "    ids=[\"new_doc1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd08f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='new_doc1', metadata={'source': 'mydata.txt'}, page_content='안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼께요')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 추가된 데이터를 확인\n",
    "db_doc.similarity_search(\"안녕하세요\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531a3a0",
   "metadata": {},
   "source": [
    "### 텍스트로부터 추가 (add_texts)\n",
    "\n",
    "`add_texts` 메서드는 텍스트를 임베딩하고 벡터 저장소에 추가하는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `texts` (Iterable[str]): 벡터 저장소에 추가할 텍스트 이터러블\n",
    "- `metadatas` (Optional[List[dict]]): 텍스트와 연관된 메타데이터 리스트 (선택적)\n",
    "- `ids` (Optional[List[str]]): 텍스트의 고유 식별자 리스트 (선택적)\n",
    "- `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `List[str]`: 벡터 저장소에 추가된 텍스트의 ID 리스트\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 입력받은 텍스트 이터러블을 리스트로 변환합니다.\n",
    "2. `_embed_documents` 메서드를 사용하여 텍스트를 임베딩합니다.\n",
    "3. `__add` 메서드를 호출하여 임베딩된 텍스트를 벡터 저장소에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c08a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_doc2', 'new_doc3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 신규 데이터를 추가\n",
    "db_doc.add_texts(\n",
    "    [\"이번엔 텍스트 데이터를 추가합니다.\", \"추가한 2번째 텍스트 데이터 입니다.\"],\n",
    "    metadatas=[{\"source\": \"mydata.txt\"}, {\"source\": \"mydata.txt\"}],\n",
    "    ids=[\"new_doc2\", \"new_doc3\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '76c6fb3c-b5d2-4dd3-8c15-4b69151472c9',\n",
       " 1: 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f',\n",
       " 2: '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f',\n",
       " 3: 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19',\n",
       " 4: 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a',\n",
       " 5: 'abfd77cf-f48b-47b1-80b9-20ca10c77068',\n",
       " 6: 'f297e148-3c06-4cab-bc1b-84d4fd428b99',\n",
       " 7: 'b5b31528-c28e-43f8-9598-0c3637fa4c1d',\n",
       " 8: '8641f8e4-9888-405b-a3e7-ee40b7950d6f',\n",
       " 9: 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa',\n",
       " 10: 'ab324b30-7bc1-455a-955d-8025fa63b452',\n",
       " 11: '0fdfb702-0f47-46fc-aead-6ee7d621ebae',\n",
       " 12: '97ee36e7-dc58-4ba9-a661-ac5661f36619',\n",
       " 13: '0b22ee18-c720-4aee-bb0b-38b5c666a644',\n",
       " 14: 'b2b4ba5f-992c-466a-80b9-caa14110adaa',\n",
       " 15: '8a7e207f-71c9-4f88-8b47-9dfbab576420',\n",
       " 16: 'b86860c0-2596-4129-8bb2-a572de594720',\n",
       " 17: 'new_doc1',\n",
       " 18: 'new_doc2',\n",
       " 19: 'new_doc3'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 추가된 데이터를 확인\n",
    "db_doc.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998556f",
   "metadata": {},
   "source": [
    "### 문서 삭제 (Delete Documents)\n",
    "\n",
    "`delete` 메서드는 벡터 저장소에서 지정된 ID에 해당하는 문서를 삭제하는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `ids` (Optional[List[str]]): 삭제할 문서의 ID 리스트\n",
    "- `**kwargs`: 추가 키워드 인자 (이 메서드에서는 사용되지 않음)\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `Optional[bool]`: 삭제 성공 시 True, 실패 시 False, 구현되지 않은 경우 None\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 입력된 ID의 유효성을 검사합니다.\n",
    "2. 삭제할 ID에 해당하는 인덱스를 찾습니다.\n",
    "3. FAISS 인덱스에서 해당 ID를 제거합니다.\n",
    "4. 문서 저장소에서 해당 ID의 문서를 삭제합니다.\n",
    "5. 인덱스와 ID 매핑을 업데이트합니다.\n",
    "\n",
    "**주요 특징**\n",
    "\n",
    "- ID 기반 삭제로 정확한 문서 관리가 가능합니다.\n",
    "- FAISS 인덱스와 문서 저장소 양쪽에서 삭제를 수행합니다.\n",
    "- 삭제 후 인덱스 재정렬을 통해 데이터 일관성을 유지합니다.\n",
    "\n",
    "**주의사항**\n",
    "\n",
    "- 삭제 작업은 되돌릴 수 없으므로 신중하게 수행해야 합니다.\n",
    "- 동시성 제어가 구현되어 있지 않아 다중 스레드 환경에서 주의가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제용 데이터를 추가\n",
    "ids = db_doc.add_texts(\n",
    "    [\"삭제용 데이터를 추가합니다.\", \"2번째 삭제용 데이터입니다.\"],\n",
    "    metadatas=[{\"source\": \"mydata.txt\"}, {\"source\": \"mydata.txt\"}],\n",
    "    ids=[\"delete_doc1\", \"delete_doc2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3af696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delete_doc1', 'delete_doc2']\n"
     ]
    }
   ],
   "source": [
    "# 삭제할 id 를 확인\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b1c17",
   "metadata": {},
   "source": [
    "- `delete` 는 ids 를 입력하여 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '76c6fb3c-b5d2-4dd3-8c15-4b69151472c9',\n",
       " 1: 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f',\n",
       " 2: '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f',\n",
       " 3: 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19',\n",
       " 4: 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a',\n",
       " 5: 'abfd77cf-f48b-47b1-80b9-20ca10c77068',\n",
       " 6: 'f297e148-3c06-4cab-bc1b-84d4fd428b99',\n",
       " 7: 'b5b31528-c28e-43f8-9598-0c3637fa4c1d',\n",
       " 8: '8641f8e4-9888-405b-a3e7-ee40b7950d6f',\n",
       " 9: 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa',\n",
       " 10: 'ab324b30-7bc1-455a-955d-8025fa63b452',\n",
       " 11: '0fdfb702-0f47-46fc-aead-6ee7d621ebae',\n",
       " 12: '97ee36e7-dc58-4ba9-a661-ac5661f36619',\n",
       " 13: '0b22ee18-c720-4aee-bb0b-38b5c666a644',\n",
       " 14: 'b2b4ba5f-992c-466a-80b9-caa14110adaa',\n",
       " 15: '8a7e207f-71c9-4f88-8b47-9dfbab576420',\n",
       " 16: 'b86860c0-2596-4129-8bb2-a572de594720',\n",
       " 17: 'new_doc1',\n",
       " 18: 'new_doc2',\n",
       " 19: 'new_doc3',\n",
       " 20: 'delete_doc1',\n",
       " 21: 'delete_doc2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_doc.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44132441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# id 로 삭제\n",
    "db_doc.delete(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef063750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '76c6fb3c-b5d2-4dd3-8c15-4b69151472c9',\n",
       " 1: 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f',\n",
       " 2: '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f',\n",
       " 3: 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19',\n",
       " 4: 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a',\n",
       " 5: 'abfd77cf-f48b-47b1-80b9-20ca10c77068',\n",
       " 6: 'f297e148-3c06-4cab-bc1b-84d4fd428b99',\n",
       " 7: 'b5b31528-c28e-43f8-9598-0c3637fa4c1d',\n",
       " 8: '8641f8e4-9888-405b-a3e7-ee40b7950d6f',\n",
       " 9: 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa',\n",
       " 10: 'ab324b30-7bc1-455a-955d-8025fa63b452',\n",
       " 11: '0fdfb702-0f47-46fc-aead-6ee7d621ebae',\n",
       " 12: '97ee36e7-dc58-4ba9-a661-ac5661f36619',\n",
       " 13: '0b22ee18-c720-4aee-bb0b-38b5c666a644',\n",
       " 14: 'b2b4ba5f-992c-466a-80b9-caa14110adaa',\n",
       " 15: '8a7e207f-71c9-4f88-8b47-9dfbab576420',\n",
       " 16: 'b86860c0-2596-4129-8bb2-a572de594720',\n",
       " 17: 'new_doc1',\n",
       " 18: 'new_doc2',\n",
       " 19: 'new_doc3'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 삭제된 결과를 출력\n",
    "db_doc.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac468473",
   "metadata": {},
   "source": [
    "## 저장 및 로드\n",
    "\n",
    "### 로컬 저장 (Save Local)\n",
    "\n",
    "`save_local` 메서드는 FAISS 인덱스, 문서 저장소, 그리고 인덱스-문서 ID 매핑을 로컬 디스크에 저장하는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `folder_path` (str): 저장할 폴더 경로\n",
    "- `index_name` (str): 저장할 인덱스 파일 이름 (기본값: \"index\")\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 지정된 폴더 경로를 생성합니다 (이미 존재하는 경우 무시).\n",
    "2. FAISS 인덱스를 별도의 파일로 저장합니다.\n",
    "3. 문서 저장소와 인덱스-문서 ID 매핑을 pickle 형식으로 저장합니다.\n",
    "\n",
    "**사용 시 고려사항**\n",
    "\n",
    "- 저장 경로에 대한 쓰기 권한이 필요합니다.\n",
    "- 대용량 데이터의 경우 저장 공간과 시간이 상당히 소요될 수 있습니다.\n",
    "- pickle 사용으로 인한 보안 위험을 고려해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 Disk 에 저장\n",
    "db_doc.save_local(folder_path=\"./faiss_db\", index_name=\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314466f",
   "metadata": {},
   "source": [
    "### 로컬에서 불러오기 (Load Local)\n",
    "\n",
    "`load_local` 클래스 메서드는 로컬 디스크에 저장된 FAISS 인덱스, 문서 저장소, 그리고 인덱스-문서 ID 매핑을 불러오는 기능을 제공합니다.\n",
    "\n",
    "**매개변수**\n",
    "\n",
    "- `folder_path` (str): 불러올 파일들이 저장된 폴더 경로\n",
    "- `embeddings` (Embeddings): 쿼리 생성에 사용할 임베딩 객체\n",
    "- `index_name` (str): 불러올 인덱스 파일 이름 (기본값: \"index\")\n",
    "- `allow_dangerous_deserialization` (bool): pickle 파일 역직렬화 허용 여부 (기본값: False)\n",
    "\n",
    "**반환값**\n",
    "\n",
    "- `FAISS`: 로드된 FAISS 객체\n",
    "\n",
    "**동작 방식**\n",
    "\n",
    "1. 역직렬화의 위험성을 확인하고 사용자의 명시적 허가를 요구합니다.\n",
    "2. FAISS 인덱스를 별도로 불러옵니다.\n",
    "3. pickle을 사용하여 문서 저장소와 인덱스-문서 ID 매핑을 불러옵니다.\n",
    "4. 불러온 데이터로 FAISS 객체를 생성하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54229bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 데이터를 로드\n",
    "loaded_db = FAISS.load_local(\n",
    "    folder_path=\"./faiss_db\",\n",
    "    index_name=\"faiss_index\",\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d4b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '76c6fb3c-b5d2-4dd3-8c15-4b69151472c9',\n",
       " 1: 'ce76172d-ae7d-45f4-a1aa-ef1a622bfe1f',\n",
       " 2: '212e54b5-22bd-4b9e-8d6c-1c256fb2dd8f',\n",
       " 3: 'ac5bac5b-b907-41d5-8c05-4f3e25c03b19',\n",
       " 4: 'c810ff52-2abb-48a3-90c5-ffb6e61d9b4a',\n",
       " 5: 'abfd77cf-f48b-47b1-80b9-20ca10c77068',\n",
       " 6: 'f297e148-3c06-4cab-bc1b-84d4fd428b99',\n",
       " 7: 'b5b31528-c28e-43f8-9598-0c3637fa4c1d',\n",
       " 8: '8641f8e4-9888-405b-a3e7-ee40b7950d6f',\n",
       " 9: 'ad7a237f-e9b7-4acb-97f4-81eb8ae524aa',\n",
       " 10: 'ab324b30-7bc1-455a-955d-8025fa63b452',\n",
       " 11: '0fdfb702-0f47-46fc-aead-6ee7d621ebae',\n",
       " 12: '97ee36e7-dc58-4ba9-a661-ac5661f36619',\n",
       " 13: '0b22ee18-c720-4aee-bb0b-38b5c666a644',\n",
       " 14: 'b2b4ba5f-992c-466a-80b9-caa14110adaa',\n",
       " 15: '8a7e207f-71c9-4f88-8b47-9dfbab576420',\n",
       " 16: 'b86860c0-2596-4129-8bb2-a572de594720',\n",
       " 17: 'new_doc1',\n",
       " 18: 'new_doc2',\n",
       " 19: 'new_doc3'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 로드된 데이터를 확인\n",
    "loaded_db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf370d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f79c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7c5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
