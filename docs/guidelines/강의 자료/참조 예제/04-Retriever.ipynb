{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b294b3be",
   "metadata": {},
   "source": [
    "# 벡터스토어 기반 검색기(VectorStoreRetriever)\n",
    "\n",
    "**VectorStore 지원 검색기** 는 vector store를 사용하여 문서를 검색하는 retriever입니다.\n",
    "\n",
    "Vector store에 구현된 **유사도 검색(similarity search)** 이나 **MMR** 과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc586a1",
   "metadata": {},
   "source": [
    "VectorStore를 생성하기에 앞서, 관련 library들을 불러옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d561f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def0eca-7f9d-4284-94e5-1cb3a13616b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4afab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e12bc9",
   "metadata": {},
   "source": [
    "실습에 사용할 파일은 'retriever/appendix-keywords.txt' 입니다.\n",
    "\n",
    "이 텍스트 파일을 1) 로드하고 2) 텍스트를 분할한 뒤 3) 임베딩하여 4) 벡터 데이터베이스에 적재해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2d1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97458d47-1e05-4f6b-a619-4ef02e190af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1228df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9b98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader를 사용하여 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/retriever/appendix-keywords.txt\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 로드된 문서를 분할합니다.\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )\n",
    "\n",
    "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000ba7b",
   "metadata": {},
   "source": [
    "### VectorStoreRetriever 초기화(as_retriever)\n",
    "`as_retriever` 메서드는 VectorStore 객체를 기반으로 vectorStoreRetriever를 초기화하고 반환합니다. 벡터 검색 기능을 활성화하여 검색기로 변환합니다.\n",
    "이 메서드를 통해 다양한 검색 옵션을 설정하여 사용자의 요구에 맞는 문서 검색을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f556cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed258d",
   "metadata": {},
   "source": [
    "사용자가 **질문(텍스트 쿼리)** 을 입력하면, 임베딩(벡터화)한 후, FAISS 데이터베이스에서 유사한 문서를 검색합니다.\n",
    "검색을 실행할 때에는 retriever의 invoke 메소드를 사용합니다.\n",
    "\n",
    "### Retriever의 invoke()\n",
    "\n",
    "`invoke` 메서드는 Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용됩니다. 이 메서드는 동기적으로 Retriever를 호출하여 주어진 쿼리에 대한 관련 문서를 반환합니다.\n",
    "\n",
    "**매개변수(parameters)**\n",
    "\n",
    "- `input`: 검색 쿼리 문자열\n",
    "- `config`: Retriever 구성 (Optional[RunnableConfig])\n",
    "- `**kwargs`: Retriever에 전달할 추가 인자\n",
    "\n",
    "**반환값(return)**\n",
    "\n",
    "- `List[Document]`: 관련 문서 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c938e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n",
      "연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "print(len(docs))\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc937f",
   "metadata": {},
   "source": [
    "이 때 parameter로 아래와 같은 다양한 옵션을 설정할 수 있습니다.\n",
    "\n",
    "**매개변수(parameters)**\n",
    "\n",
    "- `**kwargs`: 검색 함수에 전달할 키워드 인자\n",
    "  - `search_type`: 검색 유형 (\"similarity\", \"mmr\", \"similarity_score_threshold\")\n",
    "  - `search_kwargs`: 추가 검색 옵션\n",
    "    - `k`: 반환할 문서 수 (기본값: 4)\n",
    "    - `score_threshold`: similarity_score_threshold 검색의 최소 유사도 임계값\n",
    "    - `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "    - `lambda_mult`: MMR 결과의 다양성 조절 (0-1 사이, 기본값: 0.5)\n",
    "    - `filter`: 문서 메타데이터 기반 필터링\n",
    "\n",
    "**반환값(return)**\n",
    "\n",
    "- `VectorStoreRetriever`: 초기화된 VectorStoreRetriever 객체\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 다양한 검색 전략 구현 가능 (유사도, MMR, 임계값 기반)\n",
    "- MMR (Maximal Marginal Relevance) 알고리즘으로 검색 결과의 다양성 조절 가능\n",
    "- 메타데이터 필터링으로 특정 조건의 문서만 검색 가능\n",
    "- `tags` 매개변수를 통해 검색기에 태그 추가 가능\n",
    "\n",
    "**주의사항**\n",
    "\n",
    "- `search_type`과 `search_kwargs`의 적절한 조합 필요\n",
    "- MMR 사용 시 `fetch_k`와 `k` 값의 균형 조절 필요\n",
    "- `score_threshold` 설정 시 너무 높은 값은 검색 결과가 없을 수 있음\n",
    "- 필터 사용 시 데이터셋의 메타데이터 구조 정확히 파악 필요\n",
    "- `lambda_mult` 값이 0에 가까울수록 다양성이 높아지고, 1에 가까울수록 유사성이 높아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d97b85",
   "metadata": {},
   "source": [
    "반환할 문서 개수를 줄여보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591d5c9",
   "metadata": {},
   "source": [
    "### top_k 설정\n",
    "\n",
    "검색 시 사용할 `k` 와 같은 검색 키워드 인자(kwargs)를 지정할 수 있습니다.\n",
    "\n",
    "`k` 매개변수는 검색 결과에서 반환할 상위 결과의 개수를 나타냅니다.\n",
    "\n",
    "- `search_kwargs`에서 `k` 매개변수를 1로 설정하여 검색 결과로 반환할 문서의 수를 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd9c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# k 설정\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6160919",
   "metadata": {},
   "source": [
    "유사도가 특정 임계값 이상인 문서들만 반환해보겠습니다.\n",
    "\n",
    "### 유사도 점수 임계값 검색(similarity_score_threshold)\n",
    "\n",
    "유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법을 설정할 수 있습니다.\n",
    "\n",
    "임계값을 적절히 설정함으로써 **관련성이 낮은 문서를 필터링** 하고, 질의와 **가장 유사한 문서만 선별** 할 수 있습니다.\n",
    "\n",
    "- `search_type` 매개변수를 `\"similarity_score_threshold\"` 로 설정하여 유사도 점수 임계값을 기준으로 검색을 수행합니다.\n",
    "\n",
    "- `search_kwargs` 매개변수에 `{\"score_threshold\": 0.8}`를 전달하여 유사도 점수 임계값을 0.8로 설정합니다. 이는 검색 결과의 **유사도 점수가 0.8 이상인 문서만 반환됨** 을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cca1fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    # 검색 유형을 \"similarity_score_threshold 으로 설정\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    # 임계값 설정\n",
    "    search_kwargs={\"score_threshold\": 0.5},\n",
    ")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in retriever.invoke(\"Word2Vec 은 무엇인가요?\"):\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c2c",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance (MMR)\n",
    "\n",
    "`MMR(Maximal Marginal Relevance)` 방식은 쿼리에 대한 관련 항목을 검색할 때 검색된 문서의 **중복** 을 피하는 방법 중 하나입니다. \n",
    "\n",
    "단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 쿼리에 대한 **문서의 관련성** 과 이미 선택된 **문서들과의 차별성을 동시에 고려** 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5c9cb",
   "metadata": {},
   "source": [
    "- `search_type` 매개변수를 `\"mmr\"` 로 설정하여 **MMR(Maximal Marginal Relevance)** 검색 알고리즘을 사용합니다.\n",
    "- `k`: 반환할 문서 수 (기본값: 4)\n",
    "- `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "- `lambda_mult`: MMR 결과의 다양성 조절 (0~1, 기본값: 0.5, 0: 유사도 점수만 고려, 1: 다양성만 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8144a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# MMR(Maximal Marginal Relevance) 검색 유형을 지정\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6}\n",
    ")\n",
    "\n",
    "# 관련 문서를 검색합니다.\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245155a0-6e4b-496b-bcae-2147f0ac5eec",
   "metadata": {},
   "source": [
    "# MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db71c7-10f4-482a-bdb3-9b2787143101",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "\n",
    "거리 기반 벡터 데이터베이스 검색은 고차원 공간에서의 쿼리 임베딩(표현)과 '거리'를 기준으로 유사한 임베딩을 가진 문서를 찾는 방식입니다. 하지만 쿼리의 **세부적인 차이나 임베딩이 데이터의 의미를 제대로 포착하지 못할 경우, 검색 결과가 달라질 수** 있습니다. 또한, 이를 수동으로 조정하는 프롬프트 엔지니어링이나 튜닝 작업은 번거로울 수 있습니다.\n",
    "\n",
    "이런 문제를 해결하기 위해, `MultiQueryRetriever` 는 주어진 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 자동으로 생성하는 LLM(Language Learning Model)을 활용해 프롬프트 튜닝 과정을 자동화합니다.\n",
    "\n",
    "이 방식은 각각의 쿼리에 대해 관련 문서 집합을 검색하고, 모든 쿼리를 아우르는 고유한 문서들의 합집합을 추출해, 잠재적으로 관련된 더 큰 문서 집합을 얻을 수 있게 해줍니다. \n",
    "\n",
    "여러 관점에서 동일한 질문을 생성함으로써, `MultiQueryRetriever` 는 거리 기반 검색의 제한을 일정 부분 극복하고, 더욱 풍부한 검색 결과를 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55481079-6c7a-446f-bdb0-2df27ef6344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a96361-f157-42e2-87d5-5be1fb1f4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86316a0a-c71f-4f23-9fb6-19918afc432a",
   "metadata": {},
   "source": [
    "앞서 사용한 PDF 문서를 이용해 벡터DB(FAISS)를 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e02475-b7b3-4ab9-b673-24f71aa2b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "593a2ac4-b1b0-4468-b2b6-ce8acad3bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "FILE_PATH = \"./data/extract_text/AI_Paradigm_Shift_Driven_by_DeepSeek.pdf\"\n",
    "\n",
    "# PDF 로더 초기화\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = loader.load_and_split(text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a229029-a3d6-44dd-83e5-770c2235d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )\n",
    "\n",
    "# 벡터DB 생성\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# retriever 생성\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# 문서 검색\n",
    "query = \"딥시크의 등장이 Microsoft에 끼친 영향\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# 검색된 문서의 개수 출력\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d90e94-bc49-4f36-af8f-6c465a68ecca",
   "metadata": {},
   "source": [
    "검색된 결과 중 1개 문서의 내용을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0c521e-425b-4d28-80cb-5a45dbc9876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. 딥시크의 영향 전망\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# 1번 문서를 출력합니다.\n",
    "print(relevant_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579162ec-21ef-4907-992f-6b24e4f032cf",
   "metadata": {},
   "source": [
    "## 사용방법\n",
    "\n",
    "`MultiQueryRetriever` 에 사용할 LLM을 지정하고 질의 생성에 사용하면, retriever가 나머지 작업을 처리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc5d61-00c3-4462-a14d-39d8f5bc13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# ChatOpenAI 언어 모델을 초기화합니다. temperature는 0으로 설정합니다.\n",
    "llm4_mini = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=AOAI_DEPLOY_GPT4O_MINI,\n",
    "    temperature=0.0,\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03919c5a-7596-41c3-9390-50f884d1e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_retriever = MultiQueryRetriever.from_llm(  # MultiQueryRetriever를 언어 모델을 사용하여 초기화합니다.\n",
    "    # 벡터 데이터베이스의 retriever와 언어 모델을 전달합니다.\n",
    "    retriever=db.as_retriever(),\n",
    "    llm=llm4_mini,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94a62-0eff-4e2b-b4a1-e3f85428120a",
   "metadata": {},
   "source": [
    "아래는 다중 쿼리를 생성하는 중간 과정을 디버깅하기 위하여 실행하는 코드입니다.\n",
    "\n",
    "먼저 `\"langchain.retrievers.multi_query\"` 로거를 가져옵니다. \n",
    "\n",
    "이는 `logging.getLogger()` 함수를 사용하여 수행됩니다. 그 다음, 이 로거의 로그 레벨을 `INFO`로 설정하여, `INFO` 레벨 이상의 로그 메시지만 출력되도록 할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "372311dd-7973-46c6-8a70-67f4ee835145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리에 대한 로깅 설정\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5e316-f1d2-4098-acd2-af1dda913882",
   "metadata": {},
   "source": [
    "이 코드는 `retriever_from_llm` 객체의 `invoke` 메서드를 사용하여 주어진 `question`과 관련된 문서를 검색합니다. \n",
    "\n",
    "검색된 문서들은 `unique_docs`라는 변수에 저장되며, 이 변수의 길이를 확인함으로써 검색된 관련 문서의 총 개수를 알 수 있습니다. 이 과정을 통해 사용자의 질문에 대한 관련 정보를 효과적으로 찾아내고 그 양을 파악할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e11410ea-2886-4fc4-b6d5-61732b98a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['딥시크의 출현이 Microsoft에 미친 영향은 무엇인가요?  ', 'Microsoft에 대한 딥시크의 영향력은 어떻게 평가될 수 있나요?  ', '딥시크의 등장으로 인해 Microsoft는 어떤 변화나 영향을 받았나요?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "검색된 문서 개수: 9\n",
      "===============\n",
      "딥시크가 촉발한 AI 패러다임 변화와 플랫폼 정책방향\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 질문을 정의합니다.\n",
    "question = \"딥시크의 등장이 Microsoft에 끼친 영향\"\n",
    "# 문서 검색\n",
    "relevant_docs = multiquery_retriever.invoke(question)\n",
    "\n",
    "# 검색된 고유한 문서의 개수를 반환합니다.\n",
    "print(\n",
    "    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n",
    "    end=\"\\n===============\\n\",\n",
    ")\n",
    "\n",
    "# 검색된 문서의 내용을 출력합니다.\n",
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075f167-7254-416d-811c-e77b17f756bd",
   "metadata": {},
   "source": [
    "## LCEL Chain 활용하는 방법\n",
    "\n",
    "### LCEL(LangChain Expression Language)\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "`|` 기호는 [unix 파이프 연산자](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.\n",
    "\n",
    "이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c884ebd-1b5b-4b6e-94ef-a65661c12e04",
   "metadata": {},
   "source": [
    "\n",
    "- 사용자 정의 프롬프트 정의하고, 정의한 프롬프트와 함께 Chain 을 생성합니다.\n",
    "\n",
    "- Chain 은 사용자의 질문을 입력 받으면 (아래의 예제에서는) 5개의 질문을 생성한 뒤 `\"\\n\"` 구분자로 구분하여 생성된 5개 질문을 반환합니다.\n",
    "\n",
    "- runnablepassthrough : 입력을 그대로 출력, 입력 쿼리가 그대로 전달됨\n",
    "\n",
    "- StrOutputParser: LLM의 출력을 문자열로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2de36dc7-ba3f-478f-bdfe-9f1414b41fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'딥시크가 Microsoft에 미친 영향은 무엇인가요?  \\nMicrosoft에 대한 딥시크의 영향력은 어떤가요?  \\n딥시크의 출현이 Microsoft에 미친 결과는 무엇인가요?  \\nMicrosoft와 딥시크의 관계는 어떻게 형성되었나요?  \\n딥시크의 등장으로 Microsoft가 변화한 점은 무엇인가요?  '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 프롬프트 템플릿을 정의합니다.(5개의 질문을 생성하도록 프롬프트를 작성하였습니다)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language model assistant. \n",
    "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \n",
    "By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. \n",
    "Your response should be a list of values separated by new lines, eg: `foo\\nbar\\nbaz\\n`\n",
    "\n",
    "#ORIGINAL QUESTION: \n",
    "{question}\n",
    "\n",
    "#Answer in Korean:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# LLMChain을 생성합니다.\n",
    "custom_multiquery_chain = (\n",
    "    {\"question\": RunnablePassthrough()} | prompt | llm4_mini | StrOutputParser() \n",
    ")\n",
    "\n",
    "# 질문을 정의합니다.\n",
    "question = \"딥시크의 등장이 Microsoft에 끼친 영향\"\n",
    "\n",
    "# 체인을 실행하여 생성된 다중 쿼리를 확인합니다.\n",
    "multi_queries = custom_multiquery_chain.invoke(question)\n",
    "# 결과를 확인합니다.(5개 질문 생성)\n",
    "multi_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69b072-2944-4ff2-8658-1cb5f62b0ca5",
   "metadata": {},
   "source": [
    "이전에 생성한 Chain을 `MultiQueryRetriever` 에 전달하여 retrieve 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2e4332-f517-48ee-b642-15195831aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    llm=custom_multiquery_chain, retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09231c3-85b0-414a-bd69-29be6979e950",
   "metadata": {},
   "source": [
    "`MultiQueryRetriever`를 사용하여 문서를 검색하고 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb81d4c5-9612-4f28-83b8-a407d00c63f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['딥시크의 등장이 Microsoft에 미친 영향은 무엇인가요?  ', 'Microsoft에 대한 딥시크의 영향력은 어떤가요?  ', '딥시크의 출현이 Microsoft에 어떤 변화를 가져왔는지 설명해 주세요.  ', 'Microsoft에 있어 딥시크의 등장은 어떤 의미를 갖는지 궁금합니다.  ', '딥시크가 Microsoft에 미친 영향에 대해 논의해 주세요.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "검색된 문서 개수: 9\n",
      "===============\n",
      "딥시크가 촉발한 AI 패러다임 변화와 플랫폼 정책방향\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 결과\n",
    "relevant_docs = multiquery_retriever.invoke(question)\n",
    "\n",
    "# 검색된 고유한 문서의 개수를 반환합니다.\n",
    "print(\n",
    "    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n",
    "    end=\"\\n===============\\n\",\n",
    ")\n",
    "\n",
    "# 검색된 문서의 내용을 출력합니다.\n",
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d690f-caa0-4008-bb1a-11e9e3b62a82",
   "metadata": {},
   "source": [
    "# 긴 문맥 재정렬(LongContextReorder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772f72f-8e04-4f9f-9411-bcf308ec1b34",
   "metadata": {},
   "source": [
    "\n",
    "모델의 아키텍처와 상관없이, 10개 이상의 검색된 문서를 포함할 경우 성능이 상당히 저하됩니다.\n",
    "\n",
    "간단히 말해, 모델이 긴 컨텍스트 중간에 있는 관련 정보에 접근해야 할 때, 제공된 문서를 무시하는 경향이 있습니다.\n",
    "\n",
    "자세한 내용은 다음 논문을 참조하세요\n",
    "\n",
    "- https://arxiv.org/abs/2307.03172\n",
    "\n",
    "이 문제를 피하기 위해, 검색 후 문서의 순서를 재배열하여 성능 저하를 방지할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a347a-78bd-4930-99a6-b2a443944f6e",
   "metadata": {},
   "source": [
    "- `Chroma` 벡터 저장소를 사용하여 텍스트 데이터를 저장하고 검색할 수 있는 `retriever`를 생성합니다.\n",
    "- `retriever`의 `invoke` 메서드를 사용하여 주어진 쿼리에 대해 관련성이 높은 문서를 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dead3983-94f9-4325-9eff-a91d0348364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습용 AOAI 환경변수 읽기\n",
    "import os\n",
    "\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_DEPLOY_GPT4O=os.getenv(\"AOAI_DEPLOY_GPT4O\")\n",
    "AOAI_DEPLOY_GPT4O_MINI=os.getenv(\"AOAI_DEPLOY_GPT4O_MINI\")\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_DEPLOY_EMBED_3_SMALL=os.getenv(\"AOAI_DEPLOY_EMBED_3_SMALL\")\n",
    "AOAI_DEPLOY_EMBED_ADA=os.getenv(\"AOAI_DEPLOY_EMBED_ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad323e39-441c-489f-9554-42a45a62dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b51e1827-12b5-401f-a48e-14225bac1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85710182-d232-4ac0-94e5-7ccecd03eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = AzureOpenAIEmbeddings(model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,  \n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caa0ab04-b24b-4bba-8110-91ff58dac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [\n",
    "    \"이건 그냥 내가 아무렇게나 적어본 글입니다.\",\n",
    "    \"사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\",\n",
    "    \"아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\",\n",
    "    \"챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\",\n",
    "    \"챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\",\n",
    "    \"애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\",\n",
    "    \"ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\",\n",
    "    \"비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\",\n",
    "    \"ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\",\n",
    "    \"FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\",\n",
    "]\n",
    "\n",
    "\n",
    "# 검색기를 생성합니다. (K는 10으로 설정합니다)\n",
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22c4ef-d784-4473-8ccc-8dd83f0a2ecc",
   "metadata": {},
   "source": [
    "검색기에 쿼리를 입력하여 검색을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee932daa-1116-4a9c-9332-6265e29666da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'),\n",
       " Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'),\n",
       " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
       " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
       " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
       " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
       " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
       " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
       " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\"\n",
    "\n",
    "# 관련성 점수에 따라 정렬된 관련 문서를 가져옵니다.\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241dd18-28ae-4657-bda4-fff61026ae49",
   "metadata": {},
   "source": [
    "`LongContextReorder` 클래스의 인스턴스인 `reordering`을 생성합니다.\n",
    "\n",
    "- `reordering.transform_documents(docs)`를 호출하여 문서 목록 `docs`를 재정렬합니다.\n",
    "  - 덜 관련된 문서는 목록의 중간에 위치하고, 더 관련된 문서는 시작과 끝에 위치하도록 재정렬됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "367b4976-dfd5-4640-bbfe-6cf486493e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'),\n",
       " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
       " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
       " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
       " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.'),\n",
       " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
       " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
       " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
       " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서를 재정렬합니다\n",
    "# 덜 관련된 문서는 목록의 중간에 위치하고 더 관련된 요소는 시작/끝에 위치합니다.\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# 4개의 관련 문서가 시작과 끝에 위치하는지 확인합니다.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104d8f0-7f1a-48d0-86e6-50be1af3afa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf265b-29fd-48e4-9f8b-078cddd96752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
